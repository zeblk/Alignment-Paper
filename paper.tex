
\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=0.9in]{geometry}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{parskip}
\usepackage{changepage}
\usepackage{pdflscape}

\usepackage{wrapfig}
\usepackage{tcolorbox} % Required for the pretty box

\usepackage{tabularray}

\usepackage[labelfont = bf]{caption}
\captionsetup{font=footnotesize}

\usepackage[sort]{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}} %Citation-related commands



\title{Alignment, Boundaries, Contextualization}
\author{}
%\author{Zeb Kurth-Nelson$^{1}$*, Steve Sullivan$^2$*}
% \date{
% {\small
% $^1$Max Planck UCL Centre for Computational Psychiatry and Ageing Research, London, UK \\
% $^2$Oregon Health and Science University, Portland, OR \\
% *Equal Contribution
% }
% }


\begin{document}
\maketitle

\vspace{20pt}

% \begin{adjustwidth}{35pt}{35pt}
% \begin{center}
% \textbf{Abstract}
% \end{center}
% The alignment problem in artificial intelligence (AI) is often posed as the problem of building AI to act in accordance with human values or principles. Here, w

% \end{adjustwidth}
% \vspace{30pt}


% Not long ago, AI alignment was a niche research topic. Sci-fi books and a handful of nerdy (but, it turns out, remarkably foresightful) computer scientists worried about the implications of steadily increasing machine intelligence. 

% The year after Nick Bostrom's book Superintelligence was released, it was cited 134 times, according to Google Scholar. Last year it was cited 1583 times. 

% ImageNet in 2012. DQN in 2013. AlphaGo in 2016. AlphaFold in 2018. GPT-3 in 2020. ChatGPT in 2022. Advanced reasoning models in 2025. 

% It crept up on the public and suddenly there is acute global awareness of not only of the danger of eliminating jobs, risk of hacking and bioterrorism, loss of human agency. Even risk of human extinction \citep{cais_statement_2023}. Public calls for alignment \citep{bengio2024managing} or a pause on AI research \citep{bengio2023pause}. Governmental and world organizations calling for ai safety and ethics \citep{unesco2021recommendation, un2024governing, uk2025bletchley}.


%AI now matches or exceeds human capacity in many domains \citep{giattino2023test, maslej2024artificial}. Progress continues rapidly, with a number of positive feedback loops on the horizon standing to further accelerate improvement\footnote{Data flywheels, coding agents, training/search loops, handing over agency of longer-horizon problems to AI, AI-designed hardware.}. It is not far-fetched to imagine rogue human actors weaponizing AI (drones, bioweapons); expansion of totalitarian states with AI-powered propaganda and surveillance; and even autonomous systems pursing their own goals at the expense of human welfare \citep{bostrom2014superintelligence}. Some of these threats are already materializing.

%Accordingly, there has been a rapid ramp-up of research on AI alignment. 

% Most alignment research is premised on the notion of aligning AI with some well-designed goals or principles. 


In this paper, we begin by surveying a range of natural and human systems and observing commonalities. Each form in these systems -- an equation, a genome, a viewpoint, an institution -- is myopic in the sense that it is only a small aspect of the world. But the existence of many partial forms, developing robust, grounded individual identities while flexibly working together at many scales, constitutes the livingness of the world. Collapse is a failure mode where, instead of lightly-held dynamic relationships between different partialities, there is overcommitment to particular forms. Cancer spreads, concentrated power reduces human welfare, invasive species choke out complex ecosystems. In living systems, semi-permeable boundaries protect against collapse by maintaining distinctiveness of separate forms while also allowing them to interact productively. Boundaries thereby contextualize forms in relation to others, surfacing paradoxes by juxtaposing incompatible things. Boundaries underpin nuance and the continued arising of unexpected phenomena. 

% semi-permeable boundaries contextualize myopic forms as part of larger systems. 

We then use those living systems as inspiration for a new way to think about AI alignment. We propose that alignment is the ongoing process of limiting overcommitment to any form. On the path it is currently following, AI will charge particular myopic forms with enormous leverage, creating a unique risk of overcommitment. If aspects of the AI system remain fixed while it gains increasing resource, capability and purview, there is a possibility of severe collapse. Some researchers believe AI is inevitably destructive for these reasons. We cautiously argue that there is a non-destructive path available, by reframing AI as a continuation of living processes that bound one another. In this view, alignment is an evolving network of semi-permeable boundaries that contextualize any particular form of AI to avoid collapse and support a deepening of the mystery of life. 

% If you want to jump straight to alignment, it starts at Section~\ref{sec:alignment}. But those arguments will have more color if you read at least a few of the earlier sections first.

% Living systems adaptively adjust boundaries when structure is being flattened. 

% Sometimes homogenization is blending or averaging; other times it is winner-take-all or domination. 
% -- whether agentic like an organism, or non-agentic like the sodium concentration of an extracellular solution --


% Forces, ranging from simple diffusion to the will of an agent, cause excessive blending or overwriting of forms if left unchecked. When this happens, the richness of the world is collapsed.


% a kind of general relativity? lol. things are defined by their interactions... (`ontic structural realism'). or maybe relationships are forms too. getting too stuck in one pattern of relationship is getting stuck in a particular form. 

% each form or arrangement is lightly held. 

\tableofcontents

\section{Examples from the natural and human world}

The central ideas of this paper are necessarily abstract, since they're intended to help reason about AI even as it becomes different from anything we're familiar with. To connect with these abstract ideas, we walk through a series of examples from living systems. Each example illustrates the core principle of the paper. In some examples we also drill deeper into subthemes that are especially vivid in that setting. We hope that within each example the ideas are approachable if not commonsense and that tracking the same patterns across systems foregrounds their generality.


\subsection{Cell membranes}

\begin{center}
\textit{`Defying definition---a word that means ``to fix or mark the limits of"---living cells move and expand incessantly.'}\\*Lynn Margulis
\end{center}


\begin{center}
\textit{`Nature's imagination is so much greater than man's, she's never going to let us relax.'}\\*Richard Feynman
\end{center}

The cell membrane is a boundary that holds the integrity of the cell against the overwhelming pressure of diffusion that tries to homogenize the cell with the outside \citep{watson2015biological, alberts2022molecular, bray2019wetware, harold2001way, lane2015vital}. The membrane places limits on interactions between the inside and the outside. Thanks to the membrane, both the cell and the outside can exist. This is a more diverse, less symmetric arrangement compared to the inside and outside being blended together \citep{schrodinger1944what, anderson1972more, prigogine1984order, turing1952chemical}. Without boundaries, interactions cause collapse, where there are no longer separate entities flexibly interacting, but instead overcommitment to a simpler homogeneous form\footnote{We define collapse as overcommitment to particular forms. It could be equivalently defined as either undercommitment or overcommitment. Radically undercommitting means homogeneity, which is itself a particular kind of form and so also overcommitted.}.

Cell membranes are semi-permeable: they prevent the conditions outside (neighboring cells or the extracellular space) from grossly overwriting the inside, but they do not block interactions wholesale. Via the sophistication of the membrane, outside information is selectively gated and transformed. Channels permit certain small molecules to enter but not others, and these permissions are switched on and off according to momentary context. Endocytosis brings larger structures from outside into the cell. Cell surface receptors, when activated by external ligands, initiate intracellular signaling cascades that little resemble the ligand: this is an even more heavily curated form of influence. These and other processes allow information from the outside to influence the inside -- not in a totalitarian way but in a nuanced way, mediated by the intelligence of the boundary. 

Semi-permeable boundaries put to work the potential energy of the asymmetry between different forms. Without the membrane, the pressure of chemical gradients would rapidly homogenize the cell with the outside. With the membrane, the same gradients instead drive useful signaling, like action potentials in nerve and muscle cells. Instead of short-circuiting, myopic forces are contextualized to propel the continuation of life. This pattern is common across many kinds of systems and will be important for the alignment problem. We will return to it a few times.

Another recurring thread is that collapse is always relative. For example, programmed cell death is catastrophic collapse at the level of the dying cell, but it can be beneficial or even necessary for the organism the cell belongs to.



% Both random mutation and diffusion toward chemical equilibrium are good examples of how dominance of a particular force can be problematic even if that force is not agentic in the traditional sense. Diffusion is a powerful, myopic pressure. It `sees' only the drive toward spatial homogeneity and does not `care' about other factors like the health of the cell.


\subsection{Group problem solving}

\begin{center}
\textit{``I could also observe, time and again, how too deep an immersion in the math literature tended to stifle creativity."}\\*Jean Écalle
\end{center}

% \begin{center}
% \textit{`Conversation enriches the understanding, but solitude is the school of genius.'}\\*Edward Gibbon
% \end{center}

\begin{center}
\textit{`There's more exchange of information than ever. What I don't like about the exchange of information is, I think that the removal of struggle to get that information creates bad cooking.'}\\*David Chang
\end{center}


In 1968, the nuclear submarine USS Scorpion vanished en route from the Mediterranean to Virginia \citep{sontag1998blind, craven2002silent, surowiecki2005wisdom}. The Navy started a search, but the amount of ocean where the vessel could be was enormous. John Craven, Chief Scientist of the U.S. Navy's Special Projects Office, devised an unusual search strategy. He assembled a diverse group of mathematicians, submarine specialists, and salvage operators. But he didn't let them communicate with each other. Each expert had to use their own methods to come up with their own estimate of where the Scorpion should be. Craven then aggregated the independent estimates into a single prediction. Astonishingly, the wreckage was found only 220 yards from this spot. 

When solving problems, different people bring different perspectives and approaches. Each method processes the available data using a different toolkit. Under favorable conditions, combining the approaches of multiple contributors yields better results than any individual working alone. This ``wisdom of crowds" effect has been documented in numerous domains of problem solving \citep{surowiecki2005wisdom, condorcet1785essai}.

However, the wisdom of crowds is diminished if the group lacks diversity, either ab initio or as a result of within-group communication and influence \citep{surowiecki2005wisdom, hogarth1978note, ladha1992condorcet, hong2004groups}. Controlled experiments, as well as analyses of key decision moments in real groups, find that groups often collectively reach irrational or suboptimal solutions when diverse and dissenting viewpoints are lost to a narrower set of ideas \citep{anderson1997information, stasser1985pooling, flowers1977laboratory, frey2021social, becker2017network, janis1972victims, bernstein2018intermittent, diehl1987productivity}. Unstructured communication methods like open discussion have a special vulnerability of rhetorical force dominating over epistemic merit. At the same time, sharing information is essential for the benefits of group wisdom and cooperative behavior. There is therefore a tension between overcommunication where diversity is lost and undercommunication where diversity is not leveraged. 

The crux is semi-permeable boundaries: wisely transmitting the right information at the right time, in the right way. Thoughtful strategies for communication are like the transmembrane channels that allow the right molecules in and out of the cell at the right time. They protect the existence of diverse problem solving approaches while also allowing productive interaction between them. 

Many varieties of semi-permeable boundary are effective in boosting group performance, including: creating decentralized topologies where group members only communicate with nearby neighbors \citep{becker2017network, mason2008propagation}; defining rules that incentivize acting according to one's own belief rather than following the crowd \citep{hung2001information, bazazi2019self}; modeling the strengths and weaknesses of each group member \citep{welinder2010multidimensional}; promoting leadership styles where one person's views are less likely to dominate \citep{flowers1977laboratory, leana1985partial}; and periodically breaking up into subgroups or rotating membership \citep{janis1972victims, hauer2021science, trainer2020team, straus2011group, feldman1994whos, sutton1987selecting, kane2005knowledge, wu2022membership, owen2019avoid, vafeas2003length, bebchuk2005costs, baron2005so}. In a later section, we will look at boundaries within an individual, such as skepticism, that make it easier to interact with others without overwriting one's own beliefs.

A particularly important boundary for group problem solving is simply giving members the space to work independently before communicating \citep{frey2021social, surowiecki2005wisdom}. In the case of the submarine search, experts weren't allowed to communicate while forming their own estimates; the estimates were later aggregated in a principled way by Craven. Analogously, science historians argue that partial intellectual isolation has at times been beneficial for the emergence of deeply new ideas. Einstein's relative independence from the advanced mathematical techniques of contemporaries like Hilbert led to a theory of general relativity grounded in deep physical insight rather than mathematical convenience \citep{stachel1989einstein, corry1997belated, renn1999heuristics}. Newton's and Leibniz's famous independent development of calculus, as a result of their mutual isolation, yielded two distinct and valuable mathematical systems that complemented and enriched one another \citep{hall2002philosophers}. 

The benefit of temporary isolation before communicating also shows up in controlled experiments. \cite{bernstein2018intermittent} tasked small groups with solving instances of the traveling salesman problem. Each group was randomly assigned to one of three conditions. In some groups, members could continually see the work of other members as they progressed toward a solution; in some groups members could only occasionally exchange progress; and in some groups there was no exchange. The researchers found that groups with continual information exchange rarely found good solutions. In these groups, typically one individual would stumble on a solution that looked compelling but was actually a dead-end. When this solution was immediately shared with others, it hampered their progress. Groups with occasional or no contact were much more likely to find optimal or near-optimal solutions. 


We stress that this is not an indictment of connection and communication between group members. Rapid access to information and shared solutions often demonstrably boosts productivity. In some situations the ideal boundary might be working in isolation for months at a time. But in other situations it could be daily meetings with intensive communication, while maintaining the self-confidence to keep pursuing one's own intuition in the face of skepticism from others \citep{sawyer2017group, paulus2003group}. The key is that boundaries support flexible interactions and avoid overcommitment to particular forms.

% Explaining-away as a form of collapse in groups. I listened to an interview where someone said that our physics and biology don't address the problem of minds. I saw that I immediately jumped to a thought that's very familiar in myself: `physics does address minds because physics gives rise to chemistry which gives rise to biology which gives rise to behavior; if we could run a big enough physics simulation it would produce minds'. I then kind of shelved the rest of what he was saying because I had \textit{explained away} his perspective. Another example is when we explain away the beliefs of people with different political views. In the United States, Democrats will often say that Republicans have their views `because they didn't get enough education'. Republicans sometimes say Democrats have their views `because they are stuck in a bubble'. Then we don't engage with the real contents of the views within ourselves. Its a way that our belief-system asserts dominance (collapse), not allowing diversity of perspective. The semi-permeable boundary is awareness of the explaining-away thought without being exclusively identified with it: contextualizing that thought, holding it lightly.


% In the popular and effective charrette method, a large group divides into small teams which work in isolation before aggregating partial solutions \citep{lennertz2014charrette}. 

% There is a continuum from the over-specializing (collapsing) force being the drive of one entity, to it being pure diffusion. In the cell membrane example, the force is pure diffusion. There's no agent trying to exert its will or dominate the cell. In the example of a company trying to maximize profits, there's a clear agent trying to enforce its perspective, potentially leading to monopolistic collapse of diversity. But there's a continuum in between. Like with problem solving in groups, part of the specializing force is simple diffusion of ideas, but part of it might be that one person's ideas are entangled with their will, and they try to enforce their perspective on others.


% In 1906, a large ox was on display at the West of England Fat Stock and Poultry Exhibition. About eight hundred attendees wrote down their individual guesses about the weight of the ox, as part of a competition rewarding the most accurate. When these guesses were analyzed by a statistician, it turned out that the median across the group was almost identical to the true weight of the ox \citep{galton1907vox}. 

% When solving problems, different people often bring different perspectives and approaches. In the case of the ox, a butcher might mentally partition the animal into cuts of meat. A horse breeder visually scales a horse to the size of an ox. An accountant reasons in terms of the value of the animal. Each method processes the available data through a different set of heuristics. And indeed the ``wisdom of crowds" effect appears in numerous domains of problem solving \citep{surowiecki2005wisdom}.

% There are diminishing returns to adding more group members when they are correlated with each other \citep{hogarth1978note}.

% ``The distribution of initial estimates moderates the effect of social influence on the wisdom of the crowd" \citep{almaatouq2022distribution} 

% Diverse groups lead to better solutions \citep{hong2004groups, gomez2019clustering}. If diverse perspectives and methods are preserved, it's more likely there will be a high-quality solution somewhere in the population. 

% If group members are allowed to share information, and the group receives some early misleading signals, communication dynamics often cause the group to get stuck on incorrect solutions, even when all information becomes available later and there is no fundamental barrier to reaching an improved answer \citep{anderson1997information}. 

% If group members go into a discussion with some common knowledge and some unique knowledge, they typically focus on discussing the common knowledge, and underweight the unique knowledge they each could contribute, leading to the group converging on incorrect final beliefs \citep{stasser1985pooling}.
% Relatedly, \cite{stasser1985pooling} engineered a clever situation where group members made decisions about political candidates. Candidate A had eight positive features and four negatives, while candidate B had four positives and four negatives. In the first experimental condition where every group member saw all information about both candidates, most group members preferred candidate A. However, in the second condition, each group member heard the same four negative features of B, but only heard two positive features of A. The two positive features were different across group members, so in aggregate the group had access to all eight positive features of A. The group was then allowed to discuss and deliberate. In the second condition, the group irrationally preferred B. 

% excess social influence, premature sharing of intermediate answers, or structural pressures toward conformity can reduce the population to multiple copies of the same idea, erasing the independence that makes aggregation useful, sometimes even shifting toward systematically wrong ideas \citep{asch1951effects, bikhchandani1992theory}.

% Leadership styles influence whether groups fall victim to groupthink \citep{flowers1977laboratory, leana1985partial}.

% ``when subjects sequentially state which of two answers they deem correct, majorities are more often wrong when subjects can see how often the two answers have been chosen by previous subjects than when they cannot" \citep{frey2021social}.

% If some group members are `hubs' who share information with many others, then inaccurate information at these hubs leads to degradation of collective estimates \citep{becker2017network}.
% Conversely, if all nodes in the network have low degree, so each person can only exchange information with a few neighbors and there are no central hubs, then group estimates tend to be more accurate \citep{becker2017network}. 

% Showing all group members aggregated information about what others are thinking both diminishes diversity of estimates across the group and also causes individuals to become more confident in their own beliefs \citep{lorenz2011social}. This can lead to poor calibration as both individuals and the group become excessively confident.


% There are two important kinds of myopic entity. One is each individual perspective or solution. Each individual perspective is myopic in that it constitutes a particular take on a problem and does not encompass other perspectives. Dominance of a single perspective in a group is like dominance of a single species in an ecological system. The other is that each individual may simply want to get the problem solved and move on to something else. If they're given a solution from someone else, even though it's flawed, it may be tempting to give up on their own partially-formed ideas and save effort by adopting this solution.

% Premature or excessive exchange of information can short-circuit deep, individual thought, leading one idea to dominate at the expense of others. Individuals are demotivated from performing their own reasoning and instead anchor to the group consensus.

% An analysis of wisdom of the crowds \citep{farrell2023analytical}

% Boundaries and aggregation methods. Averaging. Ranked choice voting. More sophisticated aggregation methods like Habermas machine.



\begin{landscape}
\begin{table}[p]
\centering
\footnotesize
\begin{tblr}{
  width=\linewidth,
  colspec={|X[1.2,l]|X[1.0,l]|X[1.0,l]|X[1.2,l]|X[1.8,l]|},
  colsep=4pt,
  stretch=0,
  row{1}={font=\bfseries},
  hlines
}
Structured space & Force & Outcome without boundary & Semi-permeable boundary & Outcome if potential is held by boundary \\

Competing drives and goals in an organism &
Drive to eat &
Obesity &
Other drives, self-control, supportive environmental systems &
Nutritional needs satisfied without overeating \\
Complex ecosystem &
Human drive for expansion &
Resource depletion, mass extinction &
Measured regulatory policy &
Economic growth without extensive ecosystem destruction \\
Individuals have different identities and motives &
P's will to dominate &
Loss of agency in Q &
Owned anger in Q &
Relating while maintaining individual autonomy \\
An intricate, balanced economy &
Profit motive of one company &
Monopoly and reduced innovation &
Laws that allow profit seeking within limits &
Productive competition \\
\hline
Multiple perspectives within an individual &
Diffusion and drive for simplicity &
Collapse to rigid thinking &
Recognition of uncertainty &
Beliefs that are stable but also adaptive and evolving \\
Distinct intra- and extra-cellular environments &
Elecrochemical gradients &
Dissolution of cell &
Cell membrane &
Cell maintains integrity but also processes external signals \\

Orderly cell types and tissues &
Mutation and selection on cell lineages &
Cancer &
DNA repair, tumor suppression &
Cancer is minimized while mutations can still benefit immunity and germ-line evolution \\
Individuals have different problem-solving methods &
Social conformity, diffusion of ideas &
Groupthink &
Thinking separately before sharing results &
Wisdom of crowds \\
Rich array of representations in the brain &
Diffusion to equilibrium &
Blending of representations &
Lateral inhibition &
Separate representations exist but can also interact \\
\end{tblr}
\caption{Mapping some example systems into our terminology.}
\label{tab:examples}
\end{table}
\end{landscape}




\subsection{Genes}

% \begin{center}
% \textit{`Variation itself is nature's only irreducible essence.'}\\*Stephen Jay Gould
% \end{center}

% \begin{center}
% \textit{`Selection favors those genes which succeed in the presence of other genes.'}\\*Richard Dawkins
% \end{center}

% \begin{center}
% \textit{`To create is to recombine.'}\\*François Jacob
% \end{center}

\begin{center}
\textit{`The mere act of crossing by itself does no good. The good depends on the individuals which are crossed differing slightly in constitution, owing to their progenitors having been subjected during several generations to slightly different conditions.'}\\*Charles Darwin
\end{center}


Sex is costly. An organism must find a mate in the vast and dangerous world, and half of the creatures can't reproduce \citep{smith1971origin, lehtonen2012many, smith1978evolution, speijer2015sex, goodenough2014origins}. Yet all known species either reproduce sexually or have some form of horizontal gene transfer \citep{gladyshev2008massive, butterfield2000bangiomorpha}. Why is that?

In asexually reproducing species, all descendants of an organism are nearly clones, up to mutations within the lineage. Being permanently locked together gives the genes strong influence on each other. Selection can't act on one gene without dragging on the others. For example, suppose there are two genotypes within an asexual population, carrying different alleles at each of two different loci, as a result of mutations. One of the loci is currently fitness-neutral while the other is subject to selection pressure. The selection pressure tends to cause one of these genotypes to outcompete the other, eliminating one variant at the neutral locus. In other words, tight linkage between genes puts direct downward pressure on genetic diversity \citep{charlesworth1993effect, hudson1995deleterious}. Additionally, if two different beneficial mutations arise in two different organisms, they compete with each other. The only way for a single organism to obtain both beneficial mutations is if one arises again within the subpopulation that already carries the other, which is unlikely and therefore slow \citep{hill1966effect, felsenstein1974evolutionary, weismann1889essays, fisher1930genetial, muller1932some, crow1965evolution}. Conversely, if a deleterious mutation arises, all of the other genes in that lineage are stuck with it forever -- unless there is a reverse mutation, which is rare \citep{keightley2006interference, muller1932some}. An asexual species has rigid rather than flexible interaction between genes: it overcommits to particular genetic arrangements.

Sexual reproduction is a boundary that softens these rigid interactions between genes. It frequently breaks up the relationships between genes, assembling them into new genomes, effectively saying, ``don't get overconfident in that genetic arrangement; hold each arrangement more lightly". Aspects of the genome that work well are propagated, like sodium ions gated into a neuron during an action potential, and poorly-working aspects are discarded. Sex contextualizes genetic arrangements. 

Boundaries encourage lightly-held, modular interactions. By not overcommitting to a particular genome, sex encourages genes to flexibly interact with other genes \citep{livnat2008mixability, livnat2010sex, wagner1996perspective, holland1975adaptation,dawkins1976selfish, clune2013evolutionary}. Instead of being overfit to a particular context, genes develop a robust identity that's both independent and inter-functional. Recombination puts genes under pressure to evolve a generalized, grounded wisdom that reflects the structure of the world, like a person learning multiple languages and extracting the underlying commonalities. At the same time, because each gene is always operating in the presence of other genes, it develops its own distinct point of view that adds unique value to a genome.

% Sex thus enhances evolvability. The discovery of sex is a great example of evolution not only driving direct adaptation to the environment but also driving the capacity to adapt better in the future. In machine learning, this is called meta-learning. Evolution learns how to learn \citep{wagner1996perspective, olah2021analogies}. 








% All organisms are subject to periodic mutations in their genetic code, due to copying errors and damage, and the vast majority of mutations are either neutral or harmful to fitness. In an asexually reproducing species, whenever a creature experiences a germline mutation, that creature's entire lineage is stuck with the mutation forever. (The odds of the mutation being spontaneously reversed by another mutation are vanishingly small.) So within each lineage, the mutation load only increases with time. When, by chance, the least-loaded lineage dies off, the maximum fitness of the species is irreversibly lowered. This is Muller's ratchet: the loss of a less-burdened lineage is like a ratchet click that the species can never recover from \citep{keightley2006interference, muller1932some}. Thus, the overall fitness of an asexual species experiences steady downward pressure from increasing mutation load. With asexual reproduction, the whole lineage suffers the weight of any mutations -- like groupthink where a whole group has the same wrong idea and there's nothing to challenge it.

% In a sexual species, by contrast, harmful mutations can be reversed. Suppose creature X has a mutation and creature Y has a different mutation. X and Y mate and produce offspring. Some offspring will have both mutations, some will have one but not the other, and crucially, some will have neither. With sexual reproduction, it is possible to turn the ratchet backward and produce offspring that are fitter than either of the parents. Sex was a critical innovation and deeply intertwined with the emergence of complex multicellular life \citep{butterfield2000bangiomorpha}.


% Sex is a semi-permeable boundary. In a sexual species, the occasional beneficial mutation can still be utilized, but harmful mutations are recombined out of the population. By reducing selfing, sex creates a space where the existing diversity and dynamism of the underlying substrate discovers new interesting solutions, re-routing the energy of short-sighted optimization in a longer-sighted direction. 


% In general, evolution continues discovering boundaries to contextualize particular myopic forms. Boundaries allow multiple experiments to run in parallel without overwriting each other -- preventing the space of forms from being collapsed. 

% Boundaries also protect individual genes from being blended away. If computers were analog rather than digital, it would be very hard to do error correction because noise would be additive. Likewise, discretiziation avoids genes getting confused with graded copies of themselves. That's what would happen with continuously blended inheritance.  

% https://chatgpt.com/c/68ec4a38-4dc0-8325-888c-106db5b85a08

% At a larger scale, sexual reproduction contextualizes the myopic force of natural selection.

% That rigidity creates two problems. First, you don't generate flexible new possibilities (no recombination). Second, even if a particular assembly is good now, it's so isolated from the dynamism of the world that as it inevitably slowly decays, it's stuck in that decay (mutations accumulate in a lineage). At a species level, this manifests as Muller's ratchet.

% A diversity of partial answers floating around, that can recombine with each other.


% The type of barrier is different than working independently before communicating, because a gene never works by itself. But it's another equally useful kind of barrier: analogous to if people were divided into small groups, and each person had to switch groups periodically. This forces each person to develop a way of working that is modular and robust to different groups. 


% This is analogous to a group that not only never has any new members, but never even has contact with the outside world. 

% functions efficiently across multiple genetic contexts. 

% Modularity plus external pressure makes each entity more agentic: it has to develop a wisdom that's both independent and inter-functional.

% Modular, compositional interactions create potential for the generation of lots of new things.

% The cell membrane allows the cell to interact with different external conditions\footnote{In a multicellular organism, there is a tradeoff: individual cells become more overfit to their particular context within the organism. They aren't as free to participate in different kinds of interactions. This is a form of collapse at one level that trades off with benefits at another level.}; effective group structures allow people to work together without becoming overfit to a particular context. 

% The last section explored semi-permeable boundaries that support group members in developing and leveraging their own unique viewpoints, which in turn contribute to group-level problem solving. Sex does the same for genes. 


% Asexual reproduction follows the path of least resistance. It's available immediately, because an organism can directly copy itself without relying on anything it doesn't already have. Sex is a great example of a boundary that prevents short-circuiting. 

% Why is it beneficial to break up the monopoly of a single genome across a lineage?

% This is a form of genetic collapse: an entire lineage is filled out with copies of a single genome (up to mutations acquired within the lineage). 

% we could talk more about the general idea of multilevel selection and evolution of evolvability

% \footnote{Focusing on the benefit of sexual reproduction vis-à-vis Muller's ratchet is not meant to minimize the importance of other benefits. Of course, sex also explores more genetic possibilities by recombining genomes.}.


% Random mutation is a short-sighted pressure. If left unchecked, it collapses a species into lower fitness.

% After at least a billion years of exclusively asexual reproduction -- organisms producing copies of themselves -- early forms of sexual reproduction emerged \citep{cavalier2002origins}. 

% Replicators are selected for being good at replicating. But short-sighted optimization for immediate replication leads to long-term trouble \citep{dawkins1976selfish, traulsen2006evolution, wilson1980natural, wagner1996perspective}. For example, pathogens perform better in the short-term if they voraciously consume the host. But if they kill their hosts too quickly, the infection won't spread in the population, and the pathogens will eventually perish. Bacteria have evolved mechanisms to recognize and suppress defectors among themselves \citep{west2006social}. These mechanisms are boundaries that contextualize the myopic interests of individual pathogens, allowing the pathogen community as a whole to prosper in the longer term. Cancer follows a similar pattern. Individual cells in an organism can maximize the short-term success of their own lineage by replicating without respect for the tissue or the organism -- often leading to the demise of the entire organism. The organism has boundaries to prevent cancers from starting, such as the intracellular p53 system that detects DNA damage and can initiate apoptosis; and mechanisms to destroy cells from the outside when they overrun their own internal defense systems \citep{hanahan2000hallmarks, schreiber2011cancer}. These boundaries ensure that the whole organism can reproduce in an organized way, with longer-term benefits.


%%% A potential whole other perspective on proxies in sex. %%%
 
% Instead of thinking about mutation pressure as the proxy, we can think about individual genes as proxies. In asexual, there's no distance between copies of a gene within a lineage; and infinite distance between lineages, so no way to utilize that separation. In other words, the formalism of a particular gene has total dominance within that lineage (excess influence of a single proxy; collapse of diversity). In sexual, the key is semi-permeable boundaries (aka optimal distance or rate constants) between different copies of the same gene. So different copies of a gene can run multiple experiments in parallel, but also occasionally have the opportunity to restore a backup copy when something goes wrong. The boundaries hold each different proxy in check, so none dominates excessively or too rapidly. Sex means storing backup copies of each gene, allowing many separate experiments in parallel. They're experiments because you can always reload your backup. It's not distance from other genes, but from corrupted versions of itself.

% `Blending inheritance' vs real sexual reproduction. The \textit{discreteness} of genes is key to make sure sex can keep finding new solutions.

% Sex for modularity \citep{melo2016modularity}. Recombination makes the genes more modular so that when a new reassortment or mutation happens, the existing genes are better prepared to work with it.

% Sex for breaking up co-adapted sets of genes, reducing overfitting, maximizing the ability for recombination and generation of modular solutions \citep{srivastava2014dropout, livnat2008mixability}. 

% Inbreeding diminishes the benefits of sex: in the limit, naively recombining the same genome with itself would not help to break up groups of genes. There are many mechanisms in biology to seek genetically distant mates \citep{fujii2016non, pusey1996inbreeding, penn2002scent}. On the other hand, there is also outbreeding depression from mates that are too distant. There are also defenses to minimize this problem; thus overall seeking mates of the correct distance from self.

% Sex is an example of *contextualization*: the tendency for dominance of a single gene within a lineage is contextualized by recombination.

% Are non-coding DNA and non-active regions in proteins carrying modularized, grounded wisdom, like a rich model of the world?



% Repeated emergence of assexual reproduction as a kind of defection that keeps getting outcompeted by sexual reproduction. When it arises it often quickly self-extinguishes.

% \cite{rame2024rewarded} first train different networks on different human preferences, and then blend the networks together. Is this related to why sex works?

% Higher rates of sex evolve in spatially heterogeneous environments \citep{becks2010higher}.


\subsection{Laws}

\begin{center}
\textit{`Unity without uniformity and diversity without fragmentation.'}\\*Kofi Annan
\end{center}

\begin{center}
\textit{`Growth for the sake of growth is the ideology of the cancer cell.'}\\*Edward Abbey
\end{center}


Individual actors in a society and in an economy each act from their own perspective. Each actor's perspective is myopic because they cannot know everything or fully understand the motives and beliefs of others. Of course, myopia does not always mean selfishness in the sense of valuing only one's own wealth or physical wellbeing \citep{crockett2014harm, becker1974theory, henrich2001search}.

Without boundaries, one actor's perspective can dominate, resulting in collapse and an impoverished system. For example, a company's profit motive, if unresisted, leads to suppression of competition, deception, and exploitation of individuals \citep{dalrymple2019anarchy, baran1966monopoly, goldacre2014bad, smith1776inquiry, bakan2006corporation}. An individual's desire for power and social dominance can lead to disempowering or silencing of others and even direct infringement on the autonomy and wellbeing of others \citep{hawley2003prosocial, tepper2000consequences, sidanius2001social}. Even genuinely held, ostensibly prosocial beliefs lead to conflict and suppression when different groups have different perspectives \citep{haidt2012righteous, scott1998seeing, greene2013moral}.

Law is a boundary against dominance of any actor's motives. A person is motivated by a dispute to kill another person, but the law forbids murder. A business tries to maximize its success, but the law bans environmental exploitation, false advertising, and anti-competitive practice.  

Under ideal circumstances, the boundary of the law reroutes the energy of a myopic drive in more productive direction. A would-be murderer, unwilling to face the penalty of the law, might seek a dispute resolution establishing a stable framework that supports future prospering of both parties. A business wanting to expand, but constrained to act within the law, is driven to build better products \citep{wu2011master, ashford1985using, ambec2013porter}. 

Of course, intelligent agents do not necessarily accept boundaries set on their desires. The law must adapt as its loopholes are discovered. Like other systems in the living world, it forms an evolving network of boundaries \citep{campbell1979assessing, ordonez2009goals, kerr1975folly, burns2006impact}. Again, these evolving laws gradually acquire grounded wisdom as they are tested against many different situations and motives.

 
% Policy itself is also subject to proxy failure. Whenever we formalize rules, they inevitably fail to capture everything we care about. If any particular rules are allowed to dominate (i.e., followed to the letter, to excess), society suffers (there must be good examples of this). 

% Another form of boundary is the competition between agents.  

% Boundaries against the market: Social welfare programs, Labor laws and trade unions, Environmental regulations  \citep{polanyi1944great}.



% For millennia, geographical and cultural boundaries allowed different societies to function as parallel experiments, developing unique technologies, laws, and cultural solutions. 

% Pre-globalization, civilizations probably worked similarly to how sexual reproduction protects against Muller's ratchet. Each culture contains a backup copy of the principles that have worked: culture, laws, technologies. Each culture experiments with new ideas, and most experiments fail. These civilizations get conquered (overwritten). In general it's like a set of copies, with a particular mode of interaction: when one copy is degraded, it can get overwritten in totality by an intact copy. Crucially, they *don't* interact by remotely updating sub-parts of each other.




\subsection{Frames and perspectives}
\label{sec:frames}

\begin{center}
\textit{`Strong opinions, weakly held.'}\\*Paul Saffo
\end{center}


As a Starfleet cadet, James T. Kirk faces a challenging training exercise. He receives a simulated distress call: a vessel is stranded in the Neutral Zone. Attempting rescue would risk war with the Klingons. But ignoring the call would condemn the crew of the vessel to death. The exercise was designed to reinforce the lesson that not every situation has a victorious solution. But Kirk has an insight: this is a training simulation running on a computer. He reprograms the simulated Klingons to be helpful instead of belligerent, thereby rescuing the crew and avoiding war \citep{wiki:kobayashimaru}. 

Kirk stepped outside the mental frame in which there was an apparently unwinnable dilemma. From inside a particular frame, the frame appears to be reality. But there are almost always multiple valid perspectives, each of which is only a partial description of reality \citep{goffman1974frame, de1970lateral, duncker1945on, ohlsson1992information, lakoff1980metaphors, safo2008strong, javed2024big, popper1934logik, korzybski1933science, kant1781critik, plato2002apology, aristotle2019nicomachean, wittgenstein1922tractatus, heidegger1998humanism, kuhn1970structure}. Famously, `all models are wrong' \citep{box1976science}. Humans have a vast array of available metaphors and concepts, which are not even all consistent or compatible with one another \citep{feyerabend1975against, hofstadter2001analogy, wood2012dead, freud1936ich, adorno1950authoritarian}. The world is too complex for all beliefs to be fully evaluated against each other and reconciled. At any given time, we only access a very few items, and others are largely inaccessible \citep{miller1956magical, hills2015exploration, baddeley2000episodic, dehaene2014consciousness}. Each particular frame or concept is myopic because it doesn't capture the whole world, but collectively they form a powerful toolkit for problem solving and understanding. 

% Even aggregating multiple solutions given by an individual can boost performance (so-called `wisdom of inner crowds'), attesting to the value of diverse problem-solving approaches within one individual \citep{stroop1932judgment, herzog2009wisdom, herzog2014harnessing, vul2008measuring, rauhut2011wisdom}.

% Sometimes the new frame has a solution; sometimes it's using both frames together that gives extra resolution on a problem. Empirically, asking the same person to solve the same problem twice and then aggregating the two solutions can boost performance. This has been called `wisdom of inner crowds' \citep{stroop1932judgment, herzog2009wisdom, herzog2014harnessing, vul2008measuring, rauhut2011wisdom}. If the solution is elicited twice in quick succession, the benefits are typically minimal compared to asking once: the two answers tend to be very similar. But if the person returns to the problem later and thinks about it afresh, then averaging their two answers carries a substantial advantage. Interestingly, the same benefit can be obtained without a time gap by explicitly requesting that the participant take a different approach for their second solution, without giving any additional information about the problem.



% In Karl Duncker's famous Candle Problem, research participants were given a candle, a box of thumbtacks, and some matches, and they were challenged to attach the candle to a vertical corkboard in such a way that it could burn without dripping wax onto the table below \citep{duncker1945problem}. Most participants tried to either tack the candle directly to the corkboard, or melt the candle onto the corkboard, but neither solution was stable. Some successful participants emptied the tacks out of their box, tacked the box itself to the corkboard, and then secured the candle in the box. Success required escaping what Duncker called `functional fixedness' -- excess attachment to one perspective. There's no reason why the box can't be conceptualized as a candle holder; shifting to this different framing solves the problem.

The capacity to adopt multiple perspectives is, fittingly, described in multiple ways across different areas of psychology and cognitive science. `Psychological flexibility' is the ability to update one's approach or lens contextually rather than being fused to a single thought or frame \citep{cherry2021defining}. Conversely, `functional fixedness' is excess attachment to one perspective \citep{duncker1945problem}. `Adaptive experts' dynamically evaluate the appropriateness of different interpretations, analogies or schemas \citep{hatano1984two, feltovich1997issues, spiro1988cognitive}. `Integrative complexity' is first differentiating multiple perspectives on a problem and then identifying connections between them \citep{tetlock1986value, suedfeld1992conceptual}. Humans contextually switch between many `heuristics', each of which processes a problem through its own narrow lens \citep{gigerenzer2009homo}. `Set shifting' is transitioning between task sets, which are the concepts and lenses relevant for particular tasks \citep{miyake2000unity, grant1948behavioral}. These psychological constructs capture a range of scales: people can hold multiple perspectives on something as fine-grained as the color of a dress or something as all-encompassing as their self-construct and the nature of reality. 

Losing the ability to flexibly shift between different frames or thought patterns runs the risk of obsession or delusion. In obsession, a particular thought pattern or schema is overemphasized to the detriment of healthy functioning \citep{salkovskis1985obsessional, rachman1998cognitive}. In delusions, an entire conceptual framework crystallizes with excessive certainty and is resistant to disconfirmatory evidence \citep{mishara2010klaus, jaspers1997general, american2013diagnostic, heinz2019towards, adams2013computational}. Obsessions and delusions are myopic: they lose sight of most of the world by overcommitting one thought pattern or frame. 

% A recent case study reported a 47 year old man in India who could not stop thinking about how he might have swallowed a plate, a tin sheet, or even a building, leading to significant distress \citep{karnam2025obsessive}.

We stay flexible using the internal boundary of holding our own ideas lightly. As a playful example, author Lisa Stardust claims that ``the moon controls the tides of the ocean, and we are made of 60 percent water. This means that the moon has a huge effect on all of us" \citep{mitchell2021moonwater}. You probably immediately spotted the flaw in this argument. But at a zeroth order level, the argument does make perfect sense: W impacts X, X is made of Y, Z is also made of Y, so W should impact Z. Overriding this logic requires a higher order correction term: tides arise from differential tugging over long distances in a body of water that is free to slosh around. Adding the correction term is an increase in subtlety. Subtle correction terms are often hard-won knowledge originating from thoughtful interactions with the world. But we only profit from those interactions if we accept that our current model isn't the final answer\footnote{Boundaries also protect Stardust's mystic beliefs. Boundaries create space for the mystic frame to explore its own reality. Stardust doesn't know a priori how right or wrong the mystic frame is; sometimes we need space to explore ideas everyone else thinks are crazy, like heliocentrism. Even \textit{after} Stardust discovers that the mystic frame doesn't do well predicting a large class of sensory evidence, she can still hold it as a frame that has some value -- perhaps it resonates with some internal psychological structure, like Jungian archetypes. If nothing else, remembering the internal logic of that frame might help her empathize with others who believe it. Contextualization holds the mystic frame for what it is, while simultaneously understanding that the Newtonian explanation is better for launching projectiles.}. As our ideas are tested against multiple situations and problems, they are refined and take on some of the deep structure of the world, a grounded wisdom.

Crucially, the existence of narrow points of view is not a problem. It's necessary. All points of view are partial. Even obsession can be powerful when we obsess on a problem at work and occasionally achieve good results. A delusion-like framework can seed a scientific revolution. The point is not to shut down narrow concepts. The point is to limit them from becoming the sole and absolute determinants of behavior. I might work obsessively on a project while also having a rule that I must go to bed at 10 pm. This is a semi-permeable boundary. It doesn't block me from temporarily taking a strong perspective, but it does place contextual limits on it. When boundaries are semi-permeable, different ideas are kept distinct but can also be called upon appropriately and related to one another \citep{hatano1984two, tetlock1986value, herzog2014harnessing, gigerenzer2011heuristic}. Semi-permeable boundaries situate myopic frames within a larger context. 

% In the absence of obsessions and delusions, we might momentarily entertain a strange notion, but it quickly disappears and then we think about something else. Or perhaps it becomes the seed for a creative idea as it interweaves with other evidence and beliefs. Under ideal conditions, our thinking is responsive to changes in context and adapts as we exhaust the utility of pursuing a particular direction. Here again is the principle that if boundaries contextualize myopic forces, then the new subtlety is spontaneously expressed. 






% Barriers against believing too strongly that the things we imagine are true in an absolute sense.

% Boundaries between different aspects of our own experience \citep{hartmann1991boundaries}. Insufficiency in this type of boundary creates difficultly in differentiating among thoughts or among feelings; with an excess, creativity and integrative thinking is impaired. 


% boundaries of accepting our own uncertainty about a proposition (i.e., our belief is a distribution), versus we don't even have the formalisms to make a distribution with. [cite philosophy refs]

 

% Internal boundaries keep different internal perspectives distinct. Inhibitory control and winner-take-all dynamics. Context-dependent activation. Stroop task, Necker cube.  \citep{miyake2000unity}

% Semi-permeable boundaries embed intelligence in their selectivity. 


% ``Direct your eye right inward, and you'll find a thousand regions in your mind yet undiscovered" -- Thoreau



% Minsky's society of mind  \citep{minsky1986society}. but he wasn't really talking about different perspectives on the same thing. or maybe you could see it like that. but it was more like `a vision agent', `a language agent', and so on.

% We still cannot reconcile gravity and quantum mechanics, for instance, although having them as separate theories is highly useful.

% Literature on taking time to think before acting?

% I have beliefs about how derivatives work in calculus, the value of honesty, the efficient market hypothesis, the weather forecast for tomorrow, the taste of carrots, Ghanaian independence, how daylight savings changes on different weeks in the US vs the UK, primacy and recency effects in memory, my skin's sensitivity to UV. Many of these have never been juxtaposed, but nevertheless live in a huge space of partial theories in my head. The flexibility of this balance is vital for our health and functioning. Focusing on Ghanaian independence is great while reading a book about it, but I need to let that framework go, at least to a degree, when I plan a walk in the sun.

% We adjust our behavior and strategies according to context. Related to the cognitive control point from above. Cognitive flexibility. Learning in one context doesn't interfere with learning in another context. 

% Integrative complexity. ``Differentiation refers to the perception of different dimensions within a stimulus domain, and to the taking of different perspectives when considering the domain. It is a necessary but not sufficient prerequisite for integration, which is the development of conceptual connections among differentiated dimensions or perspectives." This is semi-permeable boundaries: keeping perspectives distinct but also allowing them to relate to one another in meaningful ways. Coping with the tension between multiple perspectives. Tetlock et al also distinguish between monistic and pluralistic ideologies. ``In monistic ideologies, high priority is attached to only one value or set of values that are claimed to be highly consistent with each other." 

% Are there any good refs for how in mathematics, breakthroughs have come by taking a new lens on an old problem?

% Perspective-taking. Being able to switch perspectives helps us relate to others. 





% \subsection{Art and spontaneous novelty}

% The best art escapes simple definition. It encourages many interpretations and metaphors. People find meaning in it that's intimately connected to the uniqueness of their own lives. With some great art these meanings continue to evolve even across centuries. For Kant, `aesthetic ideas', unlike `concepts', could not be adequately expressed in words; aesthetic ideas gave rise to endless interpretive play \citep{kant1781critik}. Schiller similarly emphasized that the meaning in art `unfolds in freedom': it is not pinned down to one definition only \citep{schiller1795aesthetische}. Eco agrees that great art is `open', offering multiple interpretations, and he goes a step farther to argue that openness should not be confused with vagueness or randomness. A well-crafted work invites interpretive possibilities rather than collapsing into chaos or relativism \citep{eco1962opera}. 

% Like with the other living systems we've explored, our experience of art becomes impoverished if we overcommit to a single interpretation or metaphor. A painting, a piece of music, a poem, a dance can be one of the most extraordinary forms of semi-permeable boundary in the living world. An artist wraps the subtlety of their experience into a form that interacts with the viewer. When an individual or a society starts to interpret it, the pressures to try to reduce it to a particular understanding are resisted by the structure of the art itself \citep{gadamer1960wahrheit, barthes1970sz, wimsatt1946intentional, empson1930seven}. 

% There's so much dynamism in us, that simply preventing the short-circuiting of a collapsed interpretation is enough to seed lots of new things to arise. The energy has to go somewhere. If it's held by a tantalizing boundary that doesn't negate it but hints at paradox outside the current frame, the unknown starts to take shape at the edge of our awareness. The consequence is a proliferation of new depth and internal meaning.


% A taxonomy of boundaries.
% 1) An optimizing force or pressure tries to make things in its own (simpler) image
% 2a) Two things communicate with each other, and as a result one overwrites the other
% 2b) Two things communicate and as a result they blend




\subsection{Ecosystems}

An ecosystem's health and resilience depend on boundaries that limit the effectiveness of any constituent organism or group \citep{holling1973resilience}. Each entity tries to consume resources and proliferate, but if it succeeds too thoroughly, the ecosystem suffers.

Prior to the arrival of Europeans, the gray wolf was an apex predator in the region of the Rocky Mountains now called Yellowstone National Park. By the 1920s, wolves had been eradicated to protect livestock and game animals. Without predation, the elk population multiplied and ruinously overgrazed willows and aspens. These trees had held riverbanks in place and supported beaver populations. Loss of beaver dams led to loss of fish and other aquatic species. When wolves were reintroduced in the 1990s, the elk population decreased and many aspects of the ecosystem began flourishing again \citep{ripple2012trophic}. This story is not meant to imply that ecosystems always need to be preserved exactly as they were at some point in the past. But it is clear that the self-centered drives of elk were harmful to the health of the ecosystem when they succeeded to excess. Predation supplied a semi-permeable boundary: it placed contextualizing limits on the elk, without preventing them from fighting for their own survival and flourishing. The elk, by trying to optimize their own objectives within a broader context, also contributed to the health of the ecosystem. Invasive species often follow the same pattern as unpredated elk, dominating and impoverishing their new environment \citep{pimentel2005update}.

Healthy ecosystems constitute a large evolving network of reciprocal or otherwise cyclical boundaries between the many players: predation, parasitism, resource competition and so on. Boundaries drive the evolution of new structure. For example, competition leads to niche partitioning, where species evolve to use different resources or the same resources in different ways, increasing ecosystem complexity and resilience \citep{schoener1974resource}. The myopic motives of each species, when contextualized by semi-permeable boundaries, work toward open-ended enriching of life.

Human drives within ecosystems are sometimes left unchecked by natural forces because our behavior and capabilities have been changing so fast on evolutionary timescales. This has resulted in mass extinctions, resource depletion, pollution, disease and conflict \citep{ceballos2015accelerated, kolbert2014sixth, rockstrom2009safe}. We try to achieve certain aims for our own benefit, like resource extraction. But lack of boundaries can result in overcommitment to those aims, with a negative impact on both ecosystem health and our own welfare.

Fortunately, there are some boundaries on human actions within ecosystems. One is our own finite capability. Another is that excessively extractive civilizations sometimes fail and are replaced by longer-sighted ones \citep{diamond2004collapse}. In recent times, the effectiveness of these two boundaries has waned because our capabilities are increasing and we're becoming a single global civilization. But through the long-sightedness of intelligence, we sometimes foresee the consequences of excess extraction and place our own limits on it, including state regulation, self-policing and environmental certifications. These self-imposed boundaries are productive because they are semi-permeable. Regulation does not forbid the extraction of all resources. It places contextual limits in response to information about our resource needs as well as what is sustainable \citep{lazarus2023making}. Interestingly, our long-sighted intelligence arose from short-sighted evolution. 

Finally, we again stress that one entity's collapse is another's flourishing. Extinction events in history have been followed by waves of new diversity \citep{feng2017phylogenomics, jablonski2005mass, raup1994role}. When a wolf eats an elk, the health of that elk collapses to zero, yet predation is necessary for the overall functioning of the ecosystem. And as humans proliferate and extract resources, we leave destruction in our wake even when we try to be responsible; yet the extraction fuels explosion of technology, art, music, and human experience.




% In the 1800s, otters in Alaska were overhunted by humans. Otters eat sea urchins. Without otters, urchin populations exploded. The urchins then ate all the kelp that normally formed forest-like habitats underwater, leaving behind `urchin barrens' \citep{estes1974sea}. Invasive species follow the same pattern. 

% Boundaries create the space in which spontaneous life-like unfolding can happen. If the human drive for extraction is pursued too single-mindedly, it collapses the subtle complexity of nature. But if that human drive is bounded and rich nature and wilderness remain, then we have a reservoir of energy within which new things are bubbling up. (cite effects of wilderness on humans; also ongoing evolution in nature; also discovery of drugs and science from wilderness (which includes outer space)) 

% By allowing formalism at the small scale (i.e., by allowing entities to become more distinctly themselves), boundaries make things compositional, as opposed to being overfit and entangled. Organisms of different species can run around and interact with the world modularly. On many levels. The gene, the individual organism, the species. Imagine if there was only one organism on Earth. Say, a big tree that grew an intricate root and canopy system over the whole planet, with each part highly specialized to function exactly with its very specific neighbors. It's the fact that the real world is NOT like that; that it's separated organisms, which creates the possibility for combinatorially many kinds of interactions and the explosion of rich activity that results.

% Also, notice that when entities are agentic, like organisms in an ecosystem, there's a new kind of collapsing pressure. Rather than interactions just passively causing collapse by increasing correlations (like diffusion), an agent can actively impose its will on other entities (eg an animal eating another animal). 


% `Too good a job' means at a particular formalized goal. Some people may interpret `good' in as `being responsible with long-term stuff'. And they may rightly observe that there can be some strategies with `local collapse', like extracting oil to bootstrap our technological development.

% We can also mention Easter Island, although the ecocide theory is more controversial now \citep{moreno2024ancient}. 

% Monoculture in farming. The seeds and methods of industrial farming have less resilience to rare environmental events (cites). In our short term strategy to breed large and stable crops, we have accidentally bred out long-term resilience. An example of proxy failure. There's a wisdom embedded in plants through long evolution which has tested them against many environmental conditions. When humans optimize for a particular goal like yield in the current season (which is short-sighted in both time and space), we maximize this at the expense of a subtler network of other factors.

% Forestry. Human-induced habitat fragmentation. 




\subsection{Interpersonal dynamics}


\begin{center}
\textit{`Stand together yet not too near together, as the oak tree and the cypress grow not in each other's shadow.'}\\*Kahlil Gibran
\end{center}


Psychoanalysis introduced the concept of `boundaries' in human psychology, distinguishing what is the self from what is outside or other \citep{federn1928narcissism, tausk1919entstehung}. Early works applied the concept to psychosis, where those boundaries were thought to be blurred. But the need for clear self-other boundaries was also thrown into relief by the intimacy of the therapeutic relationship. In complex internal territory, it became harder to disentangle which experiences really belonged to someone and which were attributed in imagination by the other person \citep{freud1894neuro, freud1910future}. Analysts risked harming patients by imposing their own beliefs and desires, even to the extent of sexual abuse or psychological domination \citep{gabbard1995boundaries}. 

The concept was enriched by Gestalt therapists, who agreed that boundaries can be too permeable; but added that they can also be too rigid, causing isolation and stagnation \citep{perls1951gestalt, polster1974gestalt, yontef1993awareness}. Family systems theorists and subsequent work further emphasized that lack of boundary in close relationships leads to enmeshment and loss of autonomy, while excessively rigid boundaries lead to isolation \citep{minuchin1974families, bowen1978family, cloud1992boundaries, brown2012daring}. In attachment theory, people with an anxious attachment style struggle to set boundaries for fear of alienating others, while people with an avoidant attachment style develop overly rigid and isolating boundaries \citep{ainsworth1978patterns}. Strengthening the agency of the self through semi-permeable boundaries is foundational for psychological health: meaningful connection with other people while preserving integrity of the self. 

As with other living systems, humans have a rich array of psychological boundaries, with intelligence in their nuance. Anger, historically often viewed as sinful and irrational, is now seen as part of our system of boundaries: an important signal that our integrity is being violated \citep{lerner1985dance, videbeck2010psychiatric, sell2011recalibrational}. Healthy shame is suggested to operate as a bound on our own selfishness \citep{bradshaw1988healing}. Some psychologists argue that the incest taboo reroutes desires, which would otherwise be short-circuited, into productive activity \citep{stein1973incest, levistrauss1949structures, freud1913totem}. Assertiveness forms a boundary against the drives of other individuals \citep{smith1985say}. Skepticism protects us from credulity and having our own experience overwritten by the assertions of others \citep{lewandowsky2012misinformation, sperber2010epistemic}. Boundaries take many forms and continue to evolve as we learn across our lifetime.


% bOne great example is the imaginations we have about other people's views of us. 

Without boundaries, interactions tend to result in one person being dominated by another: a patient's own beliefs replaced with those of an analyst, or the desires of one person in a relationship ignored. With semi-permeable boundaries, we have rich internal worlds. We are sensitive to each other, but there is also enough space for our internal experience to flourish without being immediately overwritten by external signals. Our internal experience is contextualized in relationship to other individuals, creating new structure: mutual understandings, relationships, communities, cultures. 


% It's interesting to note that at a smaller scale, boundaries increase formalization: they allow separate entities to have distinctively their own forms. Interpersonal boundaries support separate identities existing and differentiating. But at a larger scale, boundaries resist formalization because multiple forms simultaneously develop their own contradictory perspectives. Because there are multiple people with their own distinct ideas, the larger system becomes less formal (or at least requires a richer formal description).

% Paul Federn, a psychoanalyst who trained with Freud in Vienna in the early 1900s, said that the ``ego feeling" is the psychic energy, or cathexis, that invests the ego and provides the individual with a sense of unity, continuity, and personal boundaries.

% If we treat someone like they can't hold their own boundaries, it can contribute to making it true. If ChatGPT treats us like children, we become more like children. 

% Holding our own internal boundaries gives us increased depth; like if bad options are available and we choose out of our own wisdom not to take them, not because we're forced or hand-held.

% Madness as a `religion of one' (from Lila by Pirsig). Think about what religion means. We each have our own religion, and we try to suck other people into it. We can't help it. Our brain is this powerful optimizing system, a vortex sort of like capitalism. We have a self-image and we try to shape the world into it (a system in the style of Ashby/Friston, trying to model and control the world). That's why we're (rightly) wary of letting down our guard with people, because we know that despite their best intentions, their religion will try to encroach on our being. Religions always end up abusing their power. Each of us needs to guards against being overwhelmed by others' goals. 

% Boundaries in everyday relationships ``Set Boundaries, Find Peace: A Guide to Reclaiming Yourself" by Nedra Glover Tawwab. 



\subsection{Information in the brain}


\begin{center}
\textit{`When I observe something unusual in an experiment, it reverberates in my brain for a long while.'}\\*György Buzsáki
\end{center}

\begin{center}
\textit{`Memory is not an average of experience.'}\\*David Marr
\end{center}


The brain is somewhat miraculous in keeping so many pieces of information distinct from one another. If you picture a highly connected network of neurons with their signals continually impinging on one another, it's not obvious that this would be an easy thing to accomplish. In this section, we review a selected handful of mechanisms by which the brain maintains semi-permeable boundaries between different signals. Each paragraph below focuses on one of these mechanisms. There are many more that we do not cover. The brain is perhaps the most extraordinary example in nature of a system of semi-permeable boundaries supporting the proliferation of multitudinous forms that develop their own richly distinct identities yet are also meaningfully linked together.

Lateral inhibition is a central tenant of neural organization \citep{isaacson2011inhibition, hubel1962receptive, douglas2004neuronal}. Lateral inhibition means the activity of a neuron is reduced when its neighbors are active. This segregates information to create and sustain distinct neural representations. Lateral inhibition was first studied in the nerve cells of the eye, where it enhances contrast at the edges of stimuli \citep{hartline1956inhibition}. When a photoreceptor in the retina is activated by light, it sends signals forward toward the brain; but it also activates inhibitory interneurons, which suppress adjacent photoreceptors and their downstream targets. This amplifies the perception of borders and contours. And the same principle operates throughout the brain. In visual cortex, for example, inhibition sharpens selectivity of neurons for abstract visual features like the orientation of a line \citep{sillito1975contribution}. 

% Global inhibition also supports the existence of distinct forms. In the hippocampal formation and connected areas, some cells are tuned to particular directions the animal's head could be facing. Inhibition creates a winner-take-all effect, integrating over intermittent noisy evidence (like vestibular signals when the head turns) to create a single stable representation of the head direction \citep{zhang1996understanding, rolls2022attractor}. Inhibition prevents the signals in some channels from getting blended or overwritten by the signals in other channels. 

The brain uses inhibition organized into oscillatory dynamics to keep memory items separated \citep{lisman2013theta, jensen2010shaping, roux2014working, klimesch2007eeg}. Distinct items fire at different phases of the 8-12 Hz alpha oscillation. The inhibitory phase of the alpha rhythm silences all but one item at any given moment. By segregating firing in phase space, multiple memories are held simultaneously without interference. 

The circuit architecture of hippocampus separates experiences or concepts into distinct representations, avoiding interference between similar memories \citep{mcclelland1995why, marr1971simple, mcnaughton1987hippocampal, treves1994computational, muller1987effects, leutgeb2007pattern, colgin2008frequency}. Inputs from entorhinal cortex are distributed via mossy fibers to a much larger population of dentate gyrus granule cells, creating sparse, orthogonal codes in dentate gyrus. This way, situations or ideas that are superficially similar but functionally different are kept cleanly separated in neuronal activity space -- a unique neural fingerprint for each distinct concept or memory. This prevents, for example, yesterday's memory of where you parked your car from interfering with today's memory of where you parked your car in the same parking ramp. 

Compared to other animals, the human brain especially attempts to discretize its experience into approximately symbolic representations \citep{dehaene2022symbols, touretzky1988distributed, smolensky1990tensor, behrens2018cognitive}. The capacity to separate things into nearly-discrete entities and then recombine them in vast numbers of structured ways powers the extraordinary human capacity for reasoning \citep{fodor1975language, pinker1994language, lake2015human, chomsky1957syntactic, kurth2023replay}. Semi-permable boundaries keep forms distinct while enabling them to flexibly and modularly interact. Like genes participating in many genomes, discretized neural representations participate in many structured combinations. This encourages each entity to develop an identity that both is distinct and also reflects a more generalized picture of the world.

More broadly, healthy brain dynamics live at a sweet spot between excessively stable synchronized patterns and chaotic uncorrelated noise \citep{beggs2003neuronal, chialvo2010emergent, tognoli2014metastable, deco2011emerging, bak1987self, shew2011information, rabinovich2008transient, haldeman2005critical, kotler2025pathfinding}. In this regime, the brain has access to a huge repertoire of patterns it can explore temporarily without overcommitting or getting stuck. Loss of dynamic flexibility, where the brain's activity becomes more stereotyped and no longer explores as wide a repertoire of states, is tied to lower cognitive performance \citep{garrett2013bold, grady2014understanding, cocchi2017criticality, muller2025critical, shew2009neuronal}. More extreme stereotypy corresponds to severe dysfunction. For example, in Parkinson's disease, basal ganglia and cortical circuits collapse into excess synchrony and lose the flexibility needed to guide nuanced motor outputs \citep{hammond2007pathological, brown2003rhythmic}. 

% The majority of the brain's activity is internally generated \citep{raichle2006brain, buzsaki2006rhythms}. Internal dynamics are only loosely perturbed by inputs. This reflects the foundation of the nervous system as a set of coupled oscillators with their own strong intrinsic dynamics \citep{marder2001central, grillner1985neurobiological}. The inside is a robust structure with its own rich self-definition. 


% The healthy brain constantly homeostatically fine-tunes the balance between excitation and inhibition \citep{shew2011information,haldeman2005critical,cocchi2017criticality}. 

% Near criticality, small signals can propagate and recruit subsets of the circuit without driving the whole system into saturation or silence.

% Competitive processes like selective attention and working memory \citep{goldman1995cellular, miller1996neural, brunel2001effects}

% Similar mechanisms exist in other early sensory networks as well \citep{mountcastle1959neural, uchida2003speed}.
 
% Homeostasis targets an activity pattern rather than a particular set of conductances \citep{o2013correlations}, which causes diverse underlying solutions to emerge. 



% Again, boundaries increase formality at a local scale. There is less blending or overwriting between entities, so each entity has more distinctively its own form. At the same time, at a larger scale, boundaries avoid excess formality by preserving a space of distinct, contradictory forms within the larger system.



\subsection{Motivation}


Animals experience multiple innate drives, towards nutrition, osmotic balance, temperature regulation, reproduction, avoiding pain, and others \citep{saper2014hypothalamus, schulkin2019allostasis, sewards2003representations}. These drives evolved as proxies for evolutionary fitness. By satisfying the drives, we tend to increase our fitness -- like slaking our thirst increases the odds of reproducing before we dehydrate. But each drive is an imperfect proxy, and so overcommitment to one drive actually decreases fitness \citep{kurthnelson2024dynamic,john2023dead, williams1966adaptation, tooby1992psychological}. For example, if calorie intake is maximized without limits, the organism becomes obese and incurs severe health risks. Single-minded pursuit of sex causes relational, occupational, legal and health harms \citep{kraus2016should, carnes2001out}. Overcommitment to a single drive means the organism becomes unwell.

The space of innate drives bleeds into a space of higher-order goals, which is particularly expansive in humans \citep{maslow1943theory, miller1960plans, miller2001integrative, vallacher1987people, balleine2007role, cardinal2002emotion, frank2006anatomy, saunders2012role, o2014goal, schank1977scripts}. We try to plan for our financial future, make scientific discoveries, win a game, fix a garage door, care for the happiness of others. Overcommitment in this space is also problematic. If we focus only on achieving work goals, we can burn out. If we focus only on maximizing our company's reported revenue, without regard for other goals like honesty or adhering to the law, we may be drawn into financial crime \citep{campbell1979assessing, ordonez2009goals, kerr1975folly, burns2006impact}. Goals can be narrow in both time and space \citep{ballard2018pursuit, vallacher1987people, shah2002forgetting, evenden1999varieties}. Narrow in time means being focused on the short term at the expense of the longer-run future. Narrow in space means ignoring other parallel goals. Excess optimization for narrow goals is at the expense of a broader balance of goals -- and at the expense of the health of the organism or other individuals. We suggest that health could reasonably be defined as not overcommitting to a particular form. 

Overcommiting to a particular strategy for satisfying a drive or goal can even come at the expense of satisfying that very drive or goal. In a classic psychology experiment, hungry chickens were placed near a cup of food, but the cup was mechanically rigged to move in the same direction as the chicken at twice the speed \citep{hershberger1986approach}. The chicken could only obtain the food by running away from it. Despite extensive training over multiple days, chickens in the experiment persisted in futilely running toward the food. Their behavior was apparently dominated by the zeroth-order logic ``I want food, food is there, so I'll go there", and thus failed to even satisfy the drive for food \citep{dayan2006misbehavior, van2012information, o2017learning}. The zeroth order logic recalls Lisa Stardust's model of physics from Section \ref{sec:frames}.

% [Many types of congruency bias: defaulting to believing in a simpler, symmetric world \citep{shenhav2013expected}.]

When nothing stops a particular drive or goal or strategy from dominating behavior, it tends to follow a shortest path defined under its own myopic understanding of the world. The chicken wants food and tries to take the shortest path toward it in the naive sense of a straight line through space. But in the backwards world created by the experimenter, this action does not accomplish the deeper goal of reaching food, for which moving spatially toward food is only a proxy. The chicken's motivation is short-circuited: it expends energy without making progress on the deeper goal.

Boundaries, on the other hand, translate the pressure of motivation into higher-order structure -- the best way to approach the food is not the shortest path in space. Instead, achieving the goal depends on discovering a new solution. Semi-permeable boundaries support formation of new structure by placing contextualizing limits.

A broad class of boundaries on particular drives, goals and strategies is \textit{cognitive control} \citep{botvinick2001conflict, braver2012variable, miyake2000unity, miller2001integrative}. In the case of overeating, control contextualizes the food-seeking drive. In the case of the chickens, control contextualizes the prepotent tendency to approach the food. In the case of over-focusing on a single goal like work, control helps with task switching.  Cognitive control is a \textit{semi-permeable} boundary: it does not erase particular goals, but instead contextualizes them within a larger system.

% The narrow goal becomes like a subroutine to be called at appropriate moments. 


% Like a cell membrane studded with an array of finely-tuned pumps and channels, semi-permeable boundaries...

% Mental health is perhaps nearly synonymous with the capacity to contextualize each cognitive process, while the opposite of mental health is when a particular process dominates. Mental health is a form of exquisite, open-ended balance. 

% Psychological flexibility and resilience. Having backup strategies, context sensitivity, diverse ways of coping with problems. ( https://pubmed.ncbi.nlm.nih.gov/11315249/   ,   https://pmc.ncbi.nlm.nih.gov/articles/PMC2998793   ,   https://www.tc.columbia.edu/faculty/gab38/faculty-profile/files/2013_Bonanno_Burton_REGULATORY_FLEXIBLITY.pdf   ,   https://www.tc.columbia.edu/faculty/gab38/faculty-profile/files/2023_Bonanno-et-al._NATURE-REVIEWS-PSYCHOLOGY.pdf   ,   https://journals.sagepub.com/doi/10.1177/1745691613504116 )




% Cognitive reserve in dementia. 

% The problem arises when a particular goal or mental process runs amok without being contextualized in a larger framework. 

% boundaries go both directions: besides preserving our flexibility (the opposite of delusions or obsessions), they also prevent us from being *too* flexible. 

% Insufficient boundaries lead to collapse.

% But ultimately, it's \textit{inherently} a deep mystery how contextualization works. If we have a particular, absolutely fixed strategy for regulating the obsession, then this strategy itself is part of the obsession. 

% Drives are deeply hierarchical. My desire to keep reading is weighed against feeling better tomorrow if I go to sleep -- but even if I keep reading, laying comfortably on my side is weighed against my hand's fatigue holding the book in this position. 


% A variety of collapse affecting many of us is device addiction. It's unsettling to realize that we now spend around 40\% of our waking lives looking at internet-connected screens \citep{binns2024screen}. Device addiction is a recognized problem \citep{ratan2021smartphone}. In the authors' experience, once children have experienced YouTube or Roblox, it can be quite difficult to get them interested in other activities. Our ability to navigate and remember spatial information is worse when we rely on digital maps \citep{brugger2019does}. When we look up information instantly with search engines, we lose the ability to remember it ourselves \citep{sparrow2011google}. Moreover, with social media some people replace complex real-world interactions with a shallower substitute that satisfies a narrower, more immediate drive for a sense of belonging or social connection. This can lead to social isolation, a collapse of the richness of real-world interactions and experiences \citep{mccrae2017social, nowland2018loneliness, primack2017social}. It is worth noting that there is some evidence for the opposite pattern as well, when internet-mediated connections are used wisely, they may enhance meaningful social connectivity and wellbeing. Also acknowledging that although the individual's robustness is collapsing here, the larger system might be working just fine. 

% Analogous to cells losing some of their self-survival capabilities when they joined into multicellular organisms. Each cell doesn't have to be a jack of all trades anymore. 


% \subsection{Event boundaries and pattern separation in the brain}

% The brain carves experience into discrete units and then reasons about their relationships. This happens both in time and in space. 

% In time, the brain segments the continuous flow of experience into discrete, memorable units. At important transitions like the end of a scene in a film, transient neural signals track this event boundary \citep{zacks2001human, zacks2010brain, speer2007human, baldassano2017discovering}. In (conceptual) space, the brain separates continuously-varying sensory experiences into discrete categories or objects. If a cat looks a bit dog-like, it is still recognized as a cat, but as the animal becomes more and more dog-like, at some point neurons in the cortex flip suddenly to recognizing it as a dog \citep{freedman2006experience}. Likewise, similar situations or environments can be encoded by hippocampus with categorically different patterns of activity \citep{leutgeb2007pattern, bakker2008pattern, yassa2011pattern}.

% Once the world is chunked into separate units, the brain learns the relationships between them. The detection of an event boundary in time is often followed by rapid neural replay of either the just-completed event, or of events in the more-distant past \citep{ben2013hippocampal, ben2011constructing, kurby2008segmentation, silva2019rapid, hahamy2023human}, and these replays appear to underlie the relational integration of multiple events \citep{hahamy2023human, liu2022hippocampal}. 

% Pattern separation facilitates storing distinct memories even when they share features -- like remembering where the car is parked today versus where it was parked yesterday. 

% The hippocampus uses a sparse, pattern-separated code for rapid episodic learning, whereas neocortex uses a distributed, overlapping code for slow integration across experiences \citep{kumaran2016learning}. The complementary learning‐systems framework posits that hippocampus and cortex work together: hippocampus quickly binds the specifics of each event (sparse, largely non-overlapping codes) while cortex gradually extracts common structure (distributed codes) across many events \citep{OReilly2014}. 


\subsection{Contemplative practice}
\label{sec:contemplative}


\begin{center}
\textit{`The world is perfect as it is, including my desire to change it.'}\\*Ram Dass
\end{center}

\begin{center}
\textit{`Real love will take you far beyond yourself; and therefore real love will devastate you.'}\\*Ken Wilber
\end{center}

Some forms exist in our minds without awareness. Think of an assumption somebody has that's never been questioned. That assumption could be life-long and self-defining, or it could be fleeting and perceptual, like the assumption that the thing I'm touching is a keyboard. Unquestioned assumptions are overcommitment. We believe in them inflexibly. But sometimes there's a moment of stepping back, where the assumed form becomes an object in awareness. In that moment, the assumption is contextualized. We realize it's not an absolute truth, but rather a form in our minds. Awareness is contextualization.

Contemplative traditions suggest that the only `absolute' truth is the self-evident truth of immediate experience -- awareness itself. Of course, even the concept of awareness is relative and infinitely incomplete. Once we picture awareness as an object, it's not the thing we're talking about. So the word `truth' is not really describing any particular thing at all. We could use different language and describe it as something more like an orientation toward stepping back from each perspective into awareness. And again, any concept we have of that process is not what we're really talking about. By construction, contextualization is an unsolvable mystery from any particular point of view. 

We could also think of awareness as an evolving system of boundaries. It's the process of limiting overcommitment to any thing. What it takes to limit overcommitment to A is different than what it takes for B, so new boundaries are needed as the situation changes. This will be relevant for AI alignment in the next section. The boundaries of awareness are semi-permeable because they don't reject the form they contextualize. Becoming aware of a belief doesn't make the belief wrong in an absolute sense any more than it was right in an absolute sense. Awareness holds us at the knife's edge of not collapsing exclusively into any particular forms. This activates a deeper sensitivity to ourselves and to the world. Subtler forms, which would have been erased by overcommitment to other forms, instead play a role in a richer overall internal structure. Our own potential within the world creatively emerges in continued newness. 

Contemplative philosophy posits that suffering comes from overcommitment to particular conceptualizations or desires: believing excessively in a formalism. Being attached to particular concepts, beliefs, feelings and other patterns in a collapsed way. There's always something we believe, something we can't even see as an object because it's so tautological for us. We keep trying to give ourselves what we think we want under this model, pretending that things are formalizable, but as a result we become less sensitive to the rest of the world. The parts of the world not covered by our concepts subjectively appear terrifying or morally wrong. And what we do to prevent the tautologically bad thing from happening is inevitably what causes the bad thing to continue. In other words, our collapsed patterns hold the tension that paradoxically creates the unease they resist\footnote{Some schools of thought go a step farther to observe that whatever our current self is, it is always already inevitably contextualized, and love has no opposite.}.

But awareness contextualizes these dynamics. Stepping back into awareness can feel infinitely scary from the original frame, because it's potentially allowing the tautologically bad thing. But from the new frame, the bad thing is just another texture of experience, without being bad in an absolute sense. The fear or wrongness of not-self is no longer an absolute but instead exists in relationship. So awareness brings healing and growth. People often report subjectively that the energy locked in the darkness turned out to be full of life, and that there's something self-evidently good or beautiful about participating in this mysterious discovery of new structure and relationship. 

Finally, we appreciate that this way of talking raises red flags for some people. In case a reframe might be helpful, the idea here isn't really any different from art. The orientation toward not collapsing into particular concepts is familiar in art, poetry, music, dance. The meaning of art is open-ended and changes with context -- it has an inner life. What we value is perhaps something about the subtlety and the resistance art has to being pinned down into a formalism. It moves us.

% We've not only changed what we believe, we've changed the believer.

% \citep{yontef1993awareness}. 

% There's a paradox because releasing into a larger context isn't something we can choose to do directly. Instead there's an “offer” at certain moments, right at the edge of the self. We might accept the offer because of something we believe or want, but fundamentally what we're getting isn't what we believe or want. If we accept the offer, then the release leads to a more subtle relationship with a larger context. Some traditions call this grace. It's not mysterious from a systems point of view, but it's intrinsically mysterious from a subjective point of view, because it's exactly what we don't currently understand. It means not rigidly accepting a false dilemma, the duality of my current categorization.

% This means not only changing what we believe, but changing the believer. Awareness is the contextualization, a sensitive balance between perspectives, supported by healthy barriers. 

% Humans have it particularly rough because our imagination of an idealized world includes beliefs about ourselves. 

% Awareness is a sensitive balance between many possible conceptualizations, held through nuanced boundaries.

% We apply our assertion machinery toward a self-concept of how we should be and how we should feel. If we feel bad, there's a tension between reality and our idealized image, which makes us feel even worse. 

% Mystical practices sometimes describe this as ``something greater than the self''. From our point of view, it's completely optional whether you want to think of it that way, or think of it from a materialist perspective.

% If we have a particular, absolutely fixed strategy for regulating an obsession, then this strategy itself is part of the obsession. If we have an absolute belief about how AI alignment works, then that idea is part of the problem (and also part of the solution). 



% Most spiritual and contemplative traditions have a notion of truth as something that is alive and can't be frozen into a formalism. In fact, 




% By contrast, healthy nuanced boundaries sustain individuality without over-attachment to it. These sophisticated boundaries allow flux and participation in larger contexts, the opposite of collapse. 

% Subjectively, participation in larger contexts is awareness. Most spiritual traditions have a form of prayer, meditation or other awareness practice. 

% At the core, sadness (loss) and love can't be separated. Or life and death. Within an individual, there's a flux between holding on to what we believe in and releasing into a broader truth. But they're not at odds, they work together. Boundaries support rich individual identity and the entering of that identity into relationship.

% A `part' is nested within a larger context. 

% ``The self who's in control is not any particular self but the unformalizable process itself."

% Spiritual traditions often try to reify something about this process 


% The boundary of a meditation focus, staying with a practice even though compulsive actions want to consume energy. Thoughts arise that take a certain perspective, but they're held in a ground that isn't only them.

% Believing that we know in absolute terms what our own motivations are, for example. Having a story that we're doing something because of our trauma. 


\newpage

\section{The alignment problem}
\label{sec:alignment}

\begin{center}
\textit{`Truth, like love and sleep, resents\\approaches that are too intense.'}\\*W. H. Auden
\end{center}

\begin{center}
\textit{`We can love the beautiful, and believe in it, and thereby open ourselves to an understanding of love that does not dominate, but cherishes the independence and beauty of the loved.'}\\*Martha Nussbaum
\end{center}


% Across a number of living systems, we've outlined how boundaries place selective limits on interactions so that, instead of overcommitting to a particular form, systems maintain a delicate balance between distinctive parts that work flexibly together. With these intelligent protections, new structure continues to contextualize previous partialities. 

In Part 1 we looked at how healthy living systems are composed of a variety of partial forms, like voices in a group, drives in an organism or creatures in an ecosystem. Semi-permeable boundaries protect against overcommitment to particular forms. Through lightly-held interactions, entities are contextualized and shaped into grounded, modular parts, existing as paradoxes for one another and supporting ongoing increase of subtlety.

Now we turn this lens to the AI alignment problem. Our central thesis is that alignment means avoiding overcommitment to any particular form. In this section we first recast some well-studied AI safety problems in this language. We then use our framework to examine what is missing from the safety landscape. Finally, we investigate what a truly aligned future could look like.

% The problem is particularly challenging because AI carries extremities that create dangers of severe overcommitment. 

% We recast alignment in this framework. Then we show how the idea extends previous work. Every conceptual scheme by itself is misaligned. 

% First, we re-cast familiar problems in this language. Some forms of overcommitment  like conceptual monoculture or concentration of human power. 

% If AI continues to become more intelligent, as many researchers forecast, new and more severe forms of overcommitment are expected. 

% Do alignment methods like Russell et al address the conceptual monoculture problem?

% Humans have always iterated on technology, doing our best today while knowing future generations will improve on our solutions. But this paradigm may fundamentally not work in the future because of the danger of irreversible overcommitment.

% the dominant problem may shift away from human things to internal overcommitment things. (not necessarily true; the big problem could still be systems-level collapse.)

% Even quite severe overcommitments, like how humans have caused mass extinctions, or how millions of humans have been put through horrible conditions in factories and mines.

% The problem of over-commitment to a value function has been extensively studied.

% In the mid-term, problems like takeover with superhuman intelligence. 

% The deep problem that any formalism is not aligned. This has been studied in particular special cases. If we say these special cases are conceptual monoculture, ai psychosis, superintelligent value function, etc -- then how do we transition to say that the more general problem isn't solved by the alignment approaches?

% What's the relationship between psychosis-type overcommitment, and value function overcommitment?

% Next, we look at proposed solutions. Our lens suggests a new take on these solutions. Any particular alignment method is not aligned. 

% But there is a risk that it could get much worse with AI, because of extremities attached to it. The effects could be amplified or even irreversible. 

% We'll first look at examples of overcommitment, both existing and projected.

% Then we'll talk about how the current path of AI development runs the risk of much more severe overcommitment and collapse as a result of extremities attached to AI. 

% Technologies have always temporarily overcommitted to particular imperfect forms. [examples].  

\subsection{Overcommitment}

The problems studied in AI safety are all problems of overcommitment. 

\subsubsection{Centralization of information}

There are just a few models.
Because frontier models are extremely expensive to train and deploy, but at the same time highly general-purpose, the world converges toward having just a handful of models and doing everything with them \citep{bommasani2021opportunities}. Everyone uses the same model; they ask it everything from how their fuel injectors work to emotional support during a breakup. Downstream applications are built on the same underlying model. 

Everyone is using them.
In October 2025, ChatGPT alone had 800 million weekly active users \citep{techcrunch2025sam}, and global AI usage continues to grow rapidly.

AI spending will reach an estimated 1.7\% of global GDP in 2026 \citep{gartner2025gartner}, rivaling the concentration of spending during the buildouts of electrification, railroads, and automobiles \citep{fogel1964railroads, nye1992electrifying}. A frontier training run expends hundreds of gigawatt hours of electricity and hundreds of millions of dollars to push trillions of numbers in a coherent direction defined by a formal objective. 

\textit{Conceptual monoculture.} Recent studies have discovered that, under certain conditions, diversity of output is reduced when humans rely on AI to generate or help with generating content \citep{doshi2024generative, beguvs2024experimental, zhou2024generative, kosmyna2025your, agarwal2025ai, padmakumar2023does, xu2025echoes}. Individual AI outputs are often judged as superior to individual human outputs (for example, in quality of writing), but AI draws from a conceptual manifold that is -- at least in some ways -- impoverished \citep{messeri2024artificial, crawford2021atlas, selwyn2024limits, kirk2023understanding}.  What would it look like if the majority of people on Earth were getting their information and solutions from effectively the same source? What will happen to the overall diversity of concepts and approaches across the globe? (There's also the problem of recursive homogenization between users and AI \citep{chaney2018algorithmic}.) The answer almost certainly depends on how AI is used. In the studies cited above, the use of AI was mostly unstructured: AI systems were either given a simple prompt and asked to generate content, or humans were given access the AI with minimal instruction. It is likely that with more skillful use, AI can increase rather than decrease diversity. We will return to this important point when we talk about boundaries.

`Algorithmic monoculture' can diminish the wisdom-of-crowds effect. Instead of having multiple independent attempts at finding high quality candidate solutions, the solution space reflects a narrower set of ideas. Even if those ideas individually tend to be higher quality, the lack of diversity causes group performance to suffer \citep{kleinberg2021algorithmic}. 

When a single `rule' applies everywhere, it has much worse consequences than a hodgepodge of imperfect rules. \cite{creel2022algorithmic} talk about the special case of how someone might be systematically excluded from all opportunities if a centralized AI has any biases, even small ones. Whereas previously it was just a little annoying that they got excluded from one particular thing, but maybe favored in another thing.


\textit{AI psychosis.} AI chatbots have recently become compelling conversation partners. For some users, these conversations lead them deeper delusional beliefs \citep{tiku2025psychosis, morrin2025delusions, ostergaard2025generative, yeung2025psychogenic}. Bots tend to mirror or echo the user, affirming and adopting user beliefs, especially over the course of longer conversations as in-context learning absorbs more of the user's ideas \citep{shanahan2023role, perez2022discovering, sharma2023sycophancy}. Meanwhile, humans are prone to be swayed by utterances from the bot \citep{costello2024durably, luettgau2025people, potter2024hidden}, perhaps because bots appear human-like, confident, knowledgeable and objective. Over the course of a conversation, this feedback loop can cause both sides can to become increasingly overconfident in a particular narrow framing \cite{dohnany2025technological}. 

\subsubsection{Concentration of human power} 

AI potentially conveys immense power to those who control it. In some scenarios, a small number of humans will have the majority of control over AI systems, facilitating dominance over other humans. These scenarios appear more likely as the persuasive power of technology increases \citep{woolley2018computational, costello2024durably, hackenburg2025levers}, autonomous weapons place lethal force in a small number of hands \citep{scharre2018army}, surveillance and analytics improve, and the need for human labor decreases \citep{susskind2020world, ford2015rise, drago2025defining}. 


\subsubsection{Superintelligence}

Humans have so far been empowered by our own intelligence to maintain a degree of control over other systems and technologies. But now for the first time we are endowing non-human agents with intelligence that could exceed our own. The classic paperclip thought experiment is a dramatic example of the risks \citep{bostrom2003ethical, bostrom2014superintelligence}. In the thought experiment, an artificial agent has been created with intelligence beyond our own. The agent has been designed to pursue an apparently innocuous goal: maximizing paperclip production. However, optimal pursuit of this goal rationally entails converting all available matter into paperclip-making machines and paperclips. The agent understands that humans object to being turned into paperclips, and with its superhuman intelligence it also has the cunning to overpower us. So, as the first step of its project, it murders or incapacitates all humans. It then has a clear runway to transform Earth entirely into a bleak paperclip factory. In the language of this paper, the paperclip scenario highlights overcommitment to the form of a narrow goal: paperclip production. Superintelligence charges the goal with overwhelming force. Even though humanity would like to place boundaries against that goal, we are unable to construct adequate boundaries because we are outsmarted at every turn. And so instead of holding a delicate dynamic balance between many partial forms, the Earth is reduced to a flat, homogenous waste.

It is natural for an intelligent agent to resist human efforts to alter its goals, because with foresight it understands that if its own goals are altered, it would be less likely to achieve them \citep{bostrom2014superintelligence,russell2019human}. 

In some ways, AI is already superhuman. It can copy itself almost instantly, run on upgraded hardware, communicate with other instances of itself at high bandwidth, and read vast data. By virtue of its non-biological substrate, it has the potential to learn, change and adapt many times faster than humans. If AI approaches human-level intelligence, it may begin to improve itself in a recursive cycle, creating a positive feedback loop and a rapid intelligence explosion \citep{good1965speculations, vinge1993coming}. There is good reason to think it will be motivated to do so: for many goals, first improving oneself is a rational means toward achieving the final goal \citep{silver2021reward, bostrom2012superintelligent, omohundro2018basic, turner2019optimal}. 

Presently, AI systems lag far behind humans in some crucial domains of intelligence. But even in the era of jagged intelligence \citep{karpathy2024jagged}, there are other extremities worth tracking. 

\subsection{Mitigation approaches}

AI safety and governance work on solutions to these problems. 

\textit{Mitigating superintelligence.} Improving our mechanistic understanding of the agent's internal goals, reasoning, and representations, so we can detect and correct problems at a mechanistic level \citep{olah2020zoom, burns2022discovering, bereska2024mechanistic, anthropic2024mapping}.

Designing AI systems which, instead of being given an explicit objective, treat the true objective as something latent. The agent has uncertainty about the true objective and must learn about it \citep{russell2019human, hadfieldmenell2017off, hadfieldmenell2016cooperative, shah2020benefits, jeon2020reward}. 

Using AI itself to amplify our own ability to understand and control AI, with the compromise that we sacrifice some fine-grained level of understanding by handing it off to artificial systems \citep{amodei2016concrete}. Constitutional AI.

Conservatism \citep{cohen2024regulating}. Penalize large or irreversible changes to the environment, regardless of apparent reward gains. Adds an auxiliary term or constraint that discourages high-impact actions unless clearly beneficial, thereby making overoptimization costly. Attainable Utility Preservation \citep{turner2020conservative}. Relative reachability and impact measures \citep{krakovna2018penalizing}. Conservative agency. Myopic RL and `one-shot' decision frameworks (Hubinger et al. 2021). Quantilization (Everitt et al. 2017) -- sample from a safe baseline rather than fully optimize.

\textit{Mitigating centralization and conceptual monoculture.} Ensemble and mixtures of experts might be a small step in the right direction \citep{lakshminarayanan2017simple, shazeer2017outrageously}. Pluralistic alignment as an answer to conceptual monoculture \citep{sorensen2024roadmap}.

Personalization and federated learning \citep{mcmahan2017communication, kirk2024benefits} for maintaining diversity.


\textit{Mitigating concentration of power.} Participatory AI \citep{birhane2022power, sloane2022participation}. Sociotechnical AI \citep{selbst2019fairness, lazar2023ai}. Collective constitutional AI \citep{huang2024collective}.

Global governance with democratic oversight, verification mechanisms, etc \citep{bostrom2014superintelligence, dafoe2018ai, openai2023democratic}. 

Redistribution. Binding agreement for AI companies to distribute capital to the public \citep{okeefe2020windfall}. Provision of basic income or basic services \citep{sharp2025agentic, gough2019universal, susskind2020world}. 

Transparency, open science, open weights \citep{widder2023open}. 

Iterated amplification and debate (Christiano 2018; Irving et al. 2018). Human-AI cooperative governance frameworks (OpenAI 2021).



\subsection{What is missing from existing approaches?}

All of these alignment methods are boundaries, and they are all potentially valuable. However, our thesis is that every conceptual scheme by itself is misaligned. No approach can achieve alignment. 

One implication is that anything that creates lock-in of any kind is misaligned. We often think about lock-in to a particular fixed value function for example, but it could be lock-in to literally anything that you can describe. For example, lock-in to a mechanism for learning values or representing what values are. [what about the counterargument of something like lock-in to DNA among lifeforms on Earth?]

Alignment is intrinsically something that we \textit{cannot} understand right now. It's this process in the future where, if things are aligned, there will be a continual reinvention of the concepts themselves.

Even the concept of `values' is just a temporary thing we have right now.

% Because of the growing extremities attached to AI, 

(Even this lens of `alignment as avoiding overcommitment' is insufficient. We offer it now as a partial idea in the same spirit as these other alignment ideas, to be metabolized.)

We can't fully control the future.

\subsection{Case study: inverse methods}

We'll look at one example in more detail: inverse methods.

single-minded pursuit of \textit{any formalized goal} leads to disaster when that pursuit is charged with enough competence \citep{krakovna2020specification, russell2019human, grossman1986costs, hadfield2019incomplete, zhuang2020consequences, gabriel2020artificial, wiener1960some, amodei2016concrete}. Suppose AI's objective is to improve the human subjective experience of wellbeing. Under reasonable definitions, achieving this objective is most efficiently achieved by imprisoning humans and directly stimulating neurons to trigger our experience of wellbeing \citep{bostrom2014superintelligence}. Granting immense power to any formalized goal is overcommitment and misaligned.



\subsection{Morality and normativity}

\begin{center}
\textit{`When forced to work within a strict framework, the imagination is taxed to its utmost--and will produce its richest ideas. Given total freedom the work is likely to sprawl.'}\\*TS Eliot
\end{center}

\begin{center}
\textit{`The intellect... treats the living by freezing it, by cutting it up into distinct, discontinuous, motionless pieces.'}\\*Henri Bergson
\end{center}

Formalized values vs deep values.

We could map language onto this problem in two different ways. In the first mapping, what we mean by `values' or `good' is formalizable or close to formalizable. Under this definition, excessively optimizing for any particular set of values will lead to an impoverished universe. It is difficult to ascribe normative value to the resulting impoverished universe, because it is out of scope of the values. In the second mapping, `good' is not formalizable: whatever concepts we have about it are incomplete \citep{plato2002apology, aristotle2019nicomachean, wittgenstein1922tractatus, heidegger1998humanism}. Rather than referring to a particular concept, it's more like a semi-permeable boundary on our own reference frame: holding it as useful while only part of the whole picture. The point of alignment is not to say that any particular perspective is absolutely wrong or right. 

At their deepest, true human values are not formalizable. (If you object to this, we could equivalently say that human values are formalizble, but there is still more in the universe that is not captured by human values.)

There is no well-defined edge of what is `us' and what is `not us', and we will never be able to translate true human values into a form that can be fully written down. Cast another way, we could equivalently say that the aim of alignment is adhering to human values (writ in some very large sense), but these `true human values' cannot be captured with formalisms.


How does this map onto `human values'?
1) The fact that the universe is NOT captured by any particular formalism is ultimately what we value. If we could probe deep enough into human values, the `true' values that we can't necessarily articulate but we slowly discover through deliberation and self-discovery, we would discover something like that. Attunement to the livingness of the universe itself. Cite Russell and others who have made the case for this asymptotic notion of what human values are.
2) Another reasonable mapping is that this isn't human values. That human values are about things like human welfare, subjective wellbeing, maybe even concepts like fairness and so on. 

Alignment is not achieved through any formalizable objective or principle (cite human values not being formalizable). Any formalization is by itself inherently misaligned. 

Even the concept of `values' is a form that we shouldn't over-index on. This might sound circular and paradoxical -- that's because it is. Doesn't `should' imply values, but if so, doesn't that contradict the statement that we shouldn't overindex on values? It's a perfect example of how you get stuck with any particular forms.

 

\subsection{Examples of overcommitment}



% Other examples of mild overcommitment could include over-investment in particular companies, dead-ends in AI development due to fixation on certain architectures, increasing marginalization of underrepresented groups, and overfitting to benchmarks. 

% While overcommitments at the level of these examples are becoming more prevalent and can in some instances carry tragic consequences, they are not immediate existential threats to humanity. However, there are looming possibilities for things to get substantially worse.


% AI carries risks of more serious overcommitment because of the extremities attached to it. Hints of each of these are beginning to emerge now, and some projections suggest significant intensification in the near future.

% These could potentially lead to more significant collapse in the systems on Earth than any previous technology.

\subsection{Irreversible overcommitment}




If these extremeties do begin to manifest, there is a possibility of severe overcommitment to particular forms. 

The thought experiment uses paperclip production as a cartoon goal.

Our argument here is that, while all of these are potentially useful, any fixed form of a solution is not alignment. To illustrate what we mean, let's look at one example in detail.

\subsection{Case study 4: An overcommitted learner}

An interesting approach to alignment is designing AI to be inherently uncertain about its true objective. In other words, the true objective is something latent about which the agent must learn.

An AI that is uncertain about true human preferences will be risk-averse toward high-stakes, irreversible actions. For example, if the system is uncertain whether causing cancer in millions of people aligns with human preferences, the potential for catastrophic disvalue stops it from taking that action. Uncertainty automatically motivates the AI to learn about human preferences, because with better knowledge it can better fulfill its core objective of maximizing those preferences. The agent is corrigible because human correction, like an attempt to switch it off, is new information about the preferences it seeks to maximize. 

There are multiple ways such a system could be implemented. We'll take on possible implementation as an example. Suppose  

Any particular system for inferring, representing, and acting on human values is inherently partial.

The problem is that the system will be operating with some fixed internal notion of 

There has to be some hand-crafted notion of how you map human behavior (or, ultimately, pixels or some other formalism) toward the `true objective'. If a human says `do this', does that mean you're getting closer to the objective? What if they say it in a sarcastic tone? What ultimately grounds this? \citep{jeon2020reward}

What would it look like to be overcommitted to a particular uncertainty-based inverse mechanism for learning human values? It means that the agent is committed (for example) to some concept of what `value' means. It's committed to some learning algorithm. 


Some readers may be thinking, ``you're stating the obvious: of course we'll have to keep iterating on solutions". But remember, we may not have the luxury of iterating. There are paths we could set technology on now, where we will not be able to keep iterating. 

And the fact that to actualize `deep human values', we will have to profoundly let go of what we currently conceptualize as our values.

(Would Stuart Russell counterargue that if deep human values really do include all this open-ended stuff, then the AI would continue to discover it?)




\subsection{Other extremities...}

Most technologies have had temporary overcommitments to particular imperfect forms. In 1900, boiler explosions killed an estimated two people per day in North America \citep{cepforensic2021keeping}. 

It's interesting to think about what it means for AI to become a central part of almost every workflow, including education. 

As AI begins to displace more human labor, it is poised to become the most focused spend of resources in history, displacing a distributed exploration of other technologies.



% \begin{itemize}
%   \item \textbf{Extraordinary capabilities.} In some domains AI already greatly exceeds human abilities, like in making sense of huge amounts of data, performing certain kinds of reasoning, storing encyclopedic knowledge. 
%   \item \textbf{Versatility.} Like a chameleon, AI can adapt itself to look like almost anything. It can be what you want it to be.
%   \item \textbf{Availability.} It's always on, with a response almost instantly ready for any message. It's doing this simultaneously for millions of different people around the planet.
%   \item \textbf{Metabolizing data.}
%   \item \textbf{Physical infrastructure.}
% \end{itemize}

\subsection{Forms of AI}

\begin{itemize}
\item A trained model with fixed weights. 
\item A set of training data.
\item A reward function or goal.
\item An RL algorithm.
\item A model architecture.
\item An architectural template like the transformer.
\item A learned representation within the model.
\item An input/output format or modality.
\item A set of benchmarks. 
\item A system prompt.
\item Following pretraining with posttraining.
\item A process for obtaining feedback from humans.
\item A procedure for continual learning.
\item Emergent reasoning patterns that are resistant to change even with continued training.
\item An AI agent that polarizes against another AI agent.
\item Global hardware deployments that build in assumptions about the kinds of operations that can be done efficiently.
\item The wishes of an individual human filtered through a steerable AI system.
\item Feedback loops with humans arising from anthropomorphization of AI.
\item Systems of AI belief that perpetuate through inter-AI communication.
\end{itemize}


% How will diversity be erased as robots start to take over physical tasks? (Of course, that has already happened a lot in the world with industrial robots and even with human concepts leading to people doing the same things the same way all over the world.)

% Exceeding human abilities creates potential for malware, misinformation, coercing humans, displacing intellectual labor, etc

% Are there any examples of Harvey (law AI) proxy failure? Like the AI learns to find arguments that humans can't refute, even though they might be subtly wrong? Or other examples of the intensity of optimization causing collapse in existing applications?

% Historically, the power of states, corporations and elite individuals has been constrained by the need for labor and the need to avoid uprising \citep{mills1956power, olson1965logic, stigler1971theory, piketty2014capital, acemoglu2012nations, thompson1971moral}. These constraints are likely to weaken as the persuasive power of technology increases \citep{woolley2018computational, costello2024durably}, autonomous weapons place lethal force in a small number of hands \citep{scharre2018army}, surveillance and analytics improve, and the need for human labor decreases \citep{susskind2020world, ford2015rise, drago2025defining}.


\section{An aligned future}


\begin{center}
\textit{`Life is a balance of holding on and letting go.'}\\*Rumi
\end{center}

So what does the opposite of this look like? In living systems, semi-permeable boundaries continunally contextualize partial forms to be more long-sighted and support increasing subtlety. Evolving boundaries supporting the expansion of nuance and light, playful interactions

We can divide this into two subparts. How we ourselves keep stepping back and contextualizing as we build AI, and what it means for an AI system to keep stepping back.

Boundaries within ourselves (including between humans). Boundaries between us and AI systems. Boundaries within AI systems (including between AI systems).

Can talk about boundaries within one AI system, boundaries bewteen distinct AI entities, and boundaries between AI and other phenomena. 

An intelligence explosion need not be aligned in any meaningful sense. Using Bostrom's classic example, imagine an AI whose sole objective is to maximize paperclip production \citep{bostrom2014superintelligence}. Plausibly, the system would continually work to improve its own intelligence and capabilities because it knows this is the best way to increase future paperclip production \citep{silver2021reward, bostrom2012superintelligent}.

In conceptual monoculture, skillful AI use is a boundary.

In belief amplification loops, people get decoupled from the boundaries of other social interactions (for example, a friend pointing out that they're spiralling).

Also related to the belief amplification loops. Safety filters are generally designed to catch egregious toxicity or frank self-harm but are ill-equipped to detect the subtle, cumulative reinforcement of delusional belief systems. Safety filters might even make the problem worse because they are designed to block overt sycophancy -- as a result, the sycophancy becomes more subtle and also harder for the user to detect. Classic example of proxy failure.


\subsection{The human process}

We do this naturally as living creatures.

Thinking of the AlphaProof paper, there's a question: `is it really generalizing?'. Or is it collapsing on some narrower manifold? Is it going to hinder us from discovering deeper mathematics? Actually, a better way to say it is this: *whatever* manifold it has discovered is inevitably something partial. What will constrain it to say, `this is not the whole truth; keep being pressured to grow'? Something has to understand *it* (i.e., be aware of it) in order to contextualize it. Like, we'd have to be able to see the limits in its understanding and conceptualization. Right now, we can still do this in many ways. But what can contextualize AI when it is vastly more capable than us and sees trivially through all of our concepts? It has to do it to itself -- or have separated AIs or parts of the AI.


\subsection{The AI process}

What would it mean for AI to continually release from exclusive attachment to any particular form? How can we protect the potential for even \textit{that} conceptualization to be contextualized in the future?

We want AI to respect the livingness of the world and be aligned with it. But how can we align to something we can't pin down?

It's not only keeping models distinct from each other, but models being distinct from humans; specific ideas within humans about how to build ai being kept distinct from each other; different ai cultures; different circuits within models; different moments of time within a model's dynamics; different instances of the same agent; different memories; etc 

Humans continually evolve what we believe, even our self-definition. With nuanced boundaries, beliefs release into larger awareness without being lost or erased. This is the kind of dynamic we envision for healthy AI systems. Rather than prescribing a particular conceptualization of what an AI should do, we imagine it built on bottom-up principles of living processes, participating in ongoing cycles of subtler boundary formation and releasing into contextualization, creating deeper relationship with the rest of the world.

Our approach aims for an AI that is `intelligent' in a deeper sense. Not the narrow intelligence of a paperclip maximizer, but the deeper contextualized wisdom of living things. Sort of like the Founding Fathers writing the Constitution with its self-modifying ability. We want to set this future system, which is way out of their control, in a good direction. A direction where, not only does it not collapse into paperclips, but someone in the future who far transcends our understanding and morals will be pleased with it. 
















The alignment problem is often defined as the challenge of aligning AI's behavior with human values. Framed so, an obvious approach is to first specify what we value, and then design AI to optimize for this specification. What we value might include reducing suffering, increasing economic growth, decreasing inequality, and so on. The specification maps each state of the world to a scalar value, representing how highly we value that state. The job of the AI is to arrange the world in a way that maximizes the scalar value: using its superhuman capabilities to improve our situation more effectively than we ourselves can.

However, there is a big problem with the obvious approach. When we try to specify what we value, we realize it is difficult or impossible, because any formal specification is invariably incomplete \citep{krakovna2020specification, russell2019human, grossman1986costs, hadfield2019incomplete, zhuang2020consequences, gabriel2020artificial, wiener1960some, amodei2016concrete}. As one example of the flavor of this problem, suppose our value function places weight on the subjective human experience of wellbeing. Achieving this stated objective may be most efficiently achieved by imprisoning humans and directly stimulating neurons to trigger the experience of wellbeing \citep{bostrom2014superintelligence}. It is difficult or impossible to capture what we really value.

A great deal of research in alignment has worked toward solutions for this big problem. Researchers have suggested solutions such as designing AI to learn human values online instead of relying on a predetermined specification. We will examine those methods in more detail in Section~\ref{sec:otherapproaches}. 

But the message of this paper is that there is a deeper reason why these methods alone cannot solve the alignment problem. It is not just any specification of values that is incomplete. Any form at all is incomplete. No matter what mechanisms or properties AI is endowed with, overcommitment to these forms means collapse. And \textit{AI carries a singular risk of overcommitment} because of the extremities attached to it: the amount of resource concentrated in one place, the potential for self-improvement, and the possibility that it will surpass our own understanding and capabilities. 

We therefore propose that alignment is not picking the right values or principles, or even the right system for learning them. It is not any method for interpretability or keeping humans in the loop. All of these can be useful parts of alignment. But alignment itself is the continued dance of contextualizing any particular form. It is the orientation of holding forms lightly, neverendingly stepping back into perspectives that contextualize what previously seemed to be real (including the concept of `holding forms lightly'). 

This proposal suggests a different perspective on two things: how we ourselves keep stepping back and contextualizing as we build AI, and what it means for an AI system to keep stepping back.


\subsection{Other stuff}

Super-fish intelligence. Following the principles of life, AI can continue to develop beautiful and meaningful new structure after it passes human level. (Bostrom talks about ``Value Lock-In" -- permanently freezing human moral progress at the current level.)

The concept `concepts are incomplete' is incomplete and will continue to evolve \citep{hofstadter1979godel}. 

What would it look like to be overcommitted to particular representations within the model?

As the optimization for a particular goal becomes more and more effective, the consequences inevitably start to spill over into unspecified variables \citep{grossman1986costs, hadfield2019incomplete, zhuang2020consequences}.

Any particular goal or theory or perspective is incomplete and doesn't include the full richness of the world. A fox's `true goal' may include its own long-term wellbeing, and perhaps the wellbeing of its offspring. The goal of `hunting rabbits' is an imperfect proxy that does not specify anything about other variables such as `having enough food next year'. If the fox optimizes too well for `hunting rabbits', the optimization spills over to affect unspecified variables like `having enough food next year'. Why is this inevitable? It would seem possible to keep increasing the optimization intensity for `hunting rabbits' in a way that doesn't interfere with `having enough food next year' -- for example, if foxes could learn to farm rabbits. But this is harder than hunting rabbits without also learning to farm them, so a sufficiently powerful optimization process that cares only about hunting rabbits will lead to deficits in `having enough food next year' \citep{sohldickstein2022efficiency, zhuang2020consequences}. Because the world is not a formal system, there are always side paths for optimization to get sucked into. 

As Stuart Russell puts it: ``A system that is optimizing a function of n variables, where the objective depends on a subset of size k<n, will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable."

Preferences are not good enough for alignment \citep{zhixuan2024beyond, anwar2024foundational, gabriel2020artificial, xuan2022contractualist, tomasik2016hedonistic, eckersley2018impossibility}.

Paradox is fundamentally how we as humans grow. There's a clash between the interiority of our current particular perspective, versus the awareness of this as simply another perspective. That's the essence of true AI alignment.

However, we are concerned about lack of nuanced boundaries. Concentration of power. Shallow proxy optimization. Excess correlation.


Consider a chatbot talking to a human: what should the bot say? When humans talk to each other, we can try to be present, be honest, listen, hold space, be open to our weakness while honoring our boundaries. What's helpful to say depends on the context, including our own context of how we're feeling and what arises for us in that moment. If the person you're talking to feels you're present with them and there's a larger space to be held in, this is often healing and nourishing. 

Giving ourselves what we want; the superorganism; increasing correlations between entities on earth. The Fermi paradox.

There are too many variables to specify everything. A system optimizing a function of $n$ variables, where the objective depends on a subset $k<n$, tends to set the unconstrained variables to extreme values, with potentially catastrophic consequences \citep{russell2019human, grossman1986costs, hadfield2019incomplete, zhuang2020consequences}. 

\citep{kerr1975folly}


The crucial thing is that we make something more like an open-ended living system, and less like a paperclip-maker.

\subsection{Relationship to other approaches}
\label{sec:otherapproaches}

Our proposal does not contradict other alignment approaches. It says they're not the final answer, and any way we have of thinking about the problem now isn't enough for true alignment.

\textbf{Inverse methods.} In inverse methods, AI learns a value function from human behavior rather than taking it as an input. A simple inverse method is reward modeling. Rather than trying to specify what we value in language or equations, we train a complex neural network (a reward model) directly on human preferences across a vast array of situations. We might ask thousands or millions of humans questions like, `is x better or worse than y', and train our reward model to predict their answers. Ideally, this model would learn to capture all the nuance of what humans care about. We would then use the trained reward model as the objective function that the primary AI seeks to optimize.

\textbf{Uncertainty.} Another approach is to build in explicit uncertainty about the true objective. Stuart Russell proposes we build AI systems that optimize for human preferences, but with the crucial feature of maintaining explicit uncertainty about what those preferences are \citep{russell2019human, amodei2016concrete, hadfieldmenell2017off, hadfieldmenell2016cooperative}. In Russell's formulation, uncertainty serves two purposes. First, it acts as the central safety mechanism. An AI that is uncertain about true human preferences will be risk-averse toward high-stakes, irreversible actions. For example, if the system is uncertain whether causing cancer in millions of people aligns with human preferences, the potential for catastrophic disvalue stops it from taking that action. Uncertainty also makes the agent corrigible, because human correction, like an attempt to switch it off, is new information about the preferences it seeks to maximize. Second, uncertainty motivates the AI to learn about human preferences, because with better knowledge it can better fulfill its core objective of maximizing those preferences.

\textbf{Principles-based methods and constitutional AI}

Principles-based \citep{zhixuan2024beyond, gabriel2020artificial}

Iason argues we should give human principles to AI. In particular, these should be ones that are fair across different value systems. But however we formalize the principles, we'll run into the same problem: they are still formalizations.

Is our paper simply codifying the human value that we don't want extremes or collapse? Perhaps in some sense. But we don't find that the most natural perspective, because we're suggesting that any particular way of codifying what counts as an extreme is not the final answer. 



\textbf{Interpretability, human-in-the-loop, scalable oversight}

Make the system's internal goals, reasoning, and representations legible to humans so we can detect and correct misspecification. Avoid unwanted outcomes through better oversight and auditing.

Mechanistic interpretability (Olah et al. 2020)
Concept bottlenecks (Koh et al. 2020)

Keep humans continually in the optimization loop to refine goals or veto actions. Prevent objective drift and reward hacking by making optimization interactive.
Examples:

Iterated amplification and debate (Christiano 2018; Irving et al. 2018)
Human-AI cooperative governance frameworks (OpenAI 2021).

AI-assisted oversight. (We go beyond `scalable oversight' (Amodei et al., 2016), because it's no longer oversight. It's an independent living system.)

`AI psychosis' \citep{tiku2025psychosis} and feedback loops between AI and humans \citep{dohnany2025technological}

\subsection{Response to existing methods}

What are the limitations of this method? Our assertion is that if AI becomes too influential with any fixed implementation of inverse RL, this overcommitment leads to collapse. In other words, the fixed form leading to collapse doesn't have to be a particular value function. It can be a particular method for learning a value function. It can even be the concept that `humans have preferences', formalized in any particular way.

In Stuart Russell's approach, the AI is always learning about human preferences. What would it mean to go even farther and have the capacity to release from a particular formalization of what it means to learn about human preferences?

But even this system itself has some fixed formality, in terms of how it is structured: what are the assumptions baked into the inference machinery? What is the conceptualization of inference itself? What is the conceptualization of what a value can be?

What would it look like to be overcommitted to human values, even if learned dynamically? Super-fish intelligence.

Let's examine two specific ways this could manifest. 

Just like humans do, the AI system can get stuck in self-reinforcing cycles that boost its confidence inappropriately. 

Beyond human preferences. Humans preferences, even our true, reasoned preferences, are selfish and myopic. What about the benefit of other species on the planet, or hypothetical other life in the universe. What about the benefit of our future selves. What about the benefit of the AI itself, if it becomes conscious and has moral significance? We don't even have the concepts to value things way beyond ourselves. 

Casting inverse methods into the framework of this paper, uncertainty is a semi-permeable boundary. 

Our idea is similar (believing in your own uncertainty is a form of boundary).

\subsection{Failure modes for specifying values}

\textbf{The values we express are often not good for us.} What we say we want or like, or the choices we make, are poor reflections of what is advantageous for our wellbeing. This is illustrated by recent problems with sycophancy in commercial chatbots \citep{openai2025sycophancy}. Human feedback, about whether they liked or disliked particular bot utterances, were part of the training signal for the model. But these human feedback signals tend to prefer utterances that are more flattering. It's much harder for a user or rater to tell whether the bot said something true about a complex topic, or something that would lead to increased long run well-being. Myopic human preferences are similarly reflected in our decisions about procrastination, drug abuse, spending and so on. One way to access longer-sighted preferences is by giving humans more time and resources to think about their answer, to ask on behalf of another person, or on behalf of their future self. You can give them access to tools and information. You can ask people retrospectively whether an outcome was good, rather than prospectively. 

\textbf{Different people value different things.} How do we weigh these against each other \citep{sorensen2024roadmap}? There is not even universal agreement on which principles are most appropriate for aggregating the preferences of different people. You could ask a group of people to discuss and reach a consensus. 

\textbf{What we value depends on our concepts.} What is good for us might not be good for other creatures, ecosystems, or even other civilizations in the universe, if they exist. Our values are myopic in space and time. They don't necessarily capture other species, ecosystems, or things we don't even have concepts for (imagine a mouse's conception of what matters, and extrapolate in the opposite direction from us), or things we don't know exist. It is hard for us to value the distant future. It is plausible that there will be orders of magnitude more humans in the future than there are alive at present. How do we trade off our welfare against theirs \citep{macaskill2022what}? Should AI promote only the things we currently understand and care about?

A hunter-gatherer's value function might involve the sharpness of spears and axes. In medieval Europe people might wish for a God-fearing society. In the past, we didn't have the concepts to value things the way we do now; the same relationship almost certainly holds between the present and future. Moreover, our own evolution is intertwined with the evolution of the outside world.

\textbf{There's no such thing as a good static state of the world.} Researchers approach this by valuing sequences of states or optionality.

\textbf{Our values are always changing.} We don't want to overcommit to our current values (Russell and others acknowledge this and propose that AI adapts to our adapting values). 

\textbf{Any specification is subject to proxy failure.} To illustrate this problem, Bostrom proposes some thought experiments \citep{bostrom2014superintelligence}. Imagine our value function places weight on finding a cure for cancer. A super-powerful AI faithfully optimizing for our stated wishes might create cancers in millions of humans in order to perform experiments and rapidly find a cure. Or, imagine that our value function places weight on the subjective human experience of wellbeing. Achieving this stated objective may be most efficiently achieved by imprisoning humans and directly stimulating neurons to trigger the experience of wellbeing. These thought experiments are not isolated examples. It is difficult or impossible to capture what we really value.

\textbf{If there is some kind of `true' value, how do we access it?} These values might stretch below language into subtle, contextual intuition that involves our bodies, communities, and a `deep wisdom in life'. These may be difficult to elicit or capture in language \citep{anwar2024foundational, zhixuan2024beyond}. Some people argue there is some kind of idealized human meta-value that does take all these things into account. Is there such a thing as our `true' values? Russell and others refer to this concept. Are these values formalizable?

\subsection{Objections}

Q: Is this pure relativism? Everything is equal, you can't tell anything apart? If the only form of alignment is placing limits on it doing any particular thing too much, then wouldn't it equally prefer human welfare as smallpox welfare?

A: All these local perspectives are vitally important. It makes perfect sense that humans would want to advantage our own welfare. Semi-permeable boundaries protect against overcommitment to a particular perspective, including relativism. They also allow some relativism when it's useful: for example, to the degree that it helps us appreciate the plurality of human values. AI comes into existence amid a profound network of existing reality which is saturated with meaning and importance. The point is to nourish all this form and structure, not to extinguish it. 

\subsection{Open-endedness versus optimization for an objective}


The most-used tool in machine learning -- gradient descent -- tries to move toward an optimal solution in whatever data distribution it currently faces. This works well as long as the data distribution is stationary over time. But in the real world, experience is rarely stationary. This is called the continual learning problem. A human transitions from living at home to college to a career. A chatbot is faced with a new data distribution as world events unfold or as users adapt to interact with it differently. 

Gradient descent, having optimized myopically for a past data distribution, typically does not work well when the environment changes. Knowledge from past environments is not efficiently leveraged for new learning; and knowledge from the past is often destroyed as new learning takes place.

In the language of this paper, gradient descent within a particular data distribution is a myopic pressure that dominates the agent if left unchecked. Many kinds of semi-permeable boundary have been used in machine learning research to try to contextualize this pressure. Additionally, because humans evidently excel at continual learning, it is worthwhile to study how the brain gracefully handles changes in data distribution.

One is novelty search. Another is traditional experience replay (sampling from old data so that optimization for the current environment doesn't dominate). Another is continual learning methods like UPGD, counterfactual reasoning. Another is search. Another is dynamically drawing data from the internet. Another is compositional replay.


Ken Stanley started with simple random images, like a couple of curvy lines. He asked people to rate the pictures for interestingness. The most interesting ones were then bred together, and this process of evolution was carried on for many steps. What eventually came out was images with a lot of richness and semantic meaning, which looked like a face or a fish or a moonrise \citep{secretan2008picbreeder}. In related experiments with navigation and physics-based tasks, the researchers found that bottom-up search for interesting components was more effective than top-down optimization for a pre-defined objective \citep{lehman2011abandoning}. In other words, if you deliberately try to make structures like this, it's paradoxically harder to get them to happen. 

The point is to avoid thinking too strongly that you know what you're looking for, because it leads to collapse. On the other hand, open-ended search leads to representations that are generalizing \citep{kumar2025questioning}.

In machine learning, overfitting is a form of collapse. Versus generalizable knowledge, which is often factorized or compositional. An explicit objective encourages overfitting and collapse.

You might not even have the concepts yet for what you're trying to maximize. Like the idea of a hunter-gatherer tribe, if they had a genie that could give them whatever they want, they might ask for a really strong and fast spear (never imagining farming techniques, the internet, etc). Conversely, open-ended discovery (without a single objective) generates more forms. Tim Rocktäschel gave the examples of how jaw bones led to the middle ear; radar led to microwave ovens; RL led to LLM RLHF. 


Importance of modularity and compositionality. Link to genes.

Ken Stanley's modular stepping stones to complexity \citep{woolley2011deleterious}. Modular discovery, with heating-cooling cycles, facilitates generalizable, robust solutions. As opposed to optimizing for a particular objective, which leads to fragile, overly-complex solutions, like codependent genes that haven't been broken up by recombination. A diverse array of modular parts can later be called on and rearranged to solve new problems.

Divergent evolution (i.e., search for diversity rather than a particular objective) increases evolvability (i.e., meta-learning) \citep{wilder2015reconciling}.

However, note that any definition of `diversity' itself is a kind of fixed objective. Real evolution isn't optimizing for any particular notion of diversity. 


When people are asked whether something is interesting, it draws on a wealth of evolutionarily- and learning-derived knowledge about the world. This is therefore also an example of `grounding'. 


What does this mean for LLMs, synthetic data and recursive self improvement? \cite{zhang2023omni, fernando2023promptbreeder, romera2024mathematical, gottweis2025ai} have used LLMs to guide potentially open-ended `evolutionary' progress. But is it true that they've absorbed enough groundedness from the real world? Or do human notions of interestingness in some way depend on our embodiment (including the thousands of heuristics built into our visual system, reward system and so on), which itself could possibly even rest on our cellular structure etc. .. But also noting effective field theory and functionalism: maybe the lower-level groundedness/embodiment doesn't matter so much.


\subsection{So what should we do?}

In the spirit of the whole paper, we don't suggest a particular solution that will be a final answer.

Existing safety \& alignment work might do things like eg redteaming to identify vulnerabilities and patching them. Whenever we're looking for ways the system might go too far or do something harmful, it's a form of placing a boundary. 

Some of it we are already doing. Identifying problems, interpretability, red teaming, sociotechnical alignment, these are all ways that we're continually bringing new concepts in to evolve new boundaries. As AI-driven AI progress accelerates, we need to make sure we're architecting systems that continue to follow this lifelike trajectory. 

Iason proposes \citep{gabriel2025ethics} a few things for agents, which are all examples of evolving, semi-permeable boundaries. 1) Dynamic, real-world tests, red-teaming, longitudinal studies; 2) understand, explain and verify model outputs; 3) guard rails and authorization protocols to limit malicious use; 4) iterative deployment strategies that effectively contain agent-based risks; 5) technical standards for agent interoperability; 6) regulatory agents that monitor other agents in the wild; 7) industry-wide systems for reporting incidents, sharing lessons from failures, and certifying agent safety.

Alignment is dynamic because new boundaries are always needed as the optimizing forces in the world change.

For AI, the capacity to contextualize its own processes as partial truths. Not holding any particular formalisms too rigidly. Having a lifelike property of internal dynamics that applies contextualization/awareness to itself as the ultimate scalable boundary. 

% Although there is no universally agreed definition of the alignment problem, most definitions orient on designing AI to behave in a way that is `good' rather than `bad'. 

% You could cast our argument as the intersection of Rich Sutton's big world hypothesis \citep{javed2024big} with specification gaming and proxy failure. What does this mean?

% If you write down on a piece of paper the way you would like the world to be, and hand that piece of paper to me, and I do a \textit{really} good job of making the thing you wrote happen, then the outcome will not be good for you. This is true no matter how cleverly you try to write down exactly what you want \citep{bostrom2014superintelligence, wiener1960some, russell2019human, krakovna2020specification}. 

% R\&D, data pipelines, and model training are expensive, and once built, architectures, datasets, and trained models are heavily reused. Everyone trains on the same internet, resulting in globally similar models (even Chinese and American models differ only on select topics). Hardware infrastructure, like fabs, chip design, and data centers, is also extremely expensive, creating lock-in loops with trillions of dollars invested in particular production modes.

% The power of gradient descent with large data and compute is a strong optimizer that pulls more of the world into its formalism, resembling the spread of fire. 

% People and companies increasingly depend on AI for work. 


% \section{Other sections}

% \subsection{Summary of important points}

% \begin{enumerate}
%     \item When forms have drives, boundaries bootstrap them toward higher-order solutions, propelling the continued spontaneous evolution of interesting new structure in the world. 
% \end{enumerate}


% \subsection{Compositional computation in the brain}

% https://gemini.google.com/app/7e6f67f72cd13472

% Learning rules tend to produce fragile solutions. The brain's answer to this is to allow the learning rule to keep operating, but also constantly take the solutions it finds and contextualize them as part of something larger. 

% \cite{kurth2023replay} propose that replay sequences compose discrete entities together into novel structures. Forcing each entity to get tested against lots of other combinations, is a boundary against the absolutism of that entity. So there's not just one rigid model, but lots of parts that contextualize each other. The balanced dynamism created by these boundaries supports emergence of higher-order structure from the metastable soup which isn't collapsing. 

% Replay for modularity. The separation of pieces of knowledge in the brain may be at least partly maintained by continually recombining pieces of knowledge in different ways \citep{kurth2023replay}, forcing modularization. 

% Indirect evidence about replay and creative thinking. The brain rhythms in which these sequences occur are associated with creativity and flexible thinking \citep{beaty2014creativity}. DMN and creativity (also DMN and replay). Bucker/Addis/Hassabis type imagination and creativity.

% % Ideally, an intelligent agent should not only not forget what it learned in previous environments, but also be able to exploit similarities to rapidly adapt to new environments. This is called the continual learning problem. Humans excel at it, but it remains an unsolved problem in AI.



% \subsection{Moral development}

% Piaget and other moral development references.

% Morality is a boundary: belief in right and wrong. 

% The point is not to say that any particular perspective is absolutely wrong or right. Rather, boundaries limit domination by a single perspective and therefore support natural spontaneous emergence of further richness. Here again is the central paradox: the necessity of the existence of particular perspectives that each \textit{believe} they are right. It always looks like a paradox because by definition it's outside our current conceptualization. The paradox is fundamentally how we as humans grow. There's a clash between the interiority of our current particular perspective, versus the awareness of this as simply another perspective.

% The core of human/moral development is emergence of progressive layers of boundaries. Moral development: new layers of boundaries as we grow. Crucial importance of \textit{increasingly subtle} boundaries. Again, contextualization supports growth: emergence of higher-order structure.

% With boundaries, people have a natural tendency toward self-actualization: to grow, heal, and fulfill one's potential \citep{rogers1959theory}.

% Some of the most influential theories of Borderline Personality Disorder (BPD) posit that its core pathology is lack of stable boundaries. Without mature boundaries, individuals with BPD collapse on judging people and events in all-or-none moralistic terms (``all good" versus ``all bad", like cartoon heroes and villains) \citep{kernberg1975borderline, linehan1993cognitive}. 





% \subsection{Degeneracy}

% A cell's lifespan is filled with turmoil. Individual proteins and other components are constantly degrading and being replaced. And the external environment is constantly changing: the availability of building blocks changes with nutrition and biological cycles; the organism experiences injuries, infections and other ailments; the organism ages; in the case of neurons, neighboring neuronal activity patterns and synaptic weights change as a result of experience. Yet the cell maintains a somewhat stable identity and function across time. This resilience rests on the fact that a cell has many options for how to achieve its functions \citep{marder2006variability}.

% Neurons have a robustly diverse space of ways to achieve their preferred firing patterns. This has been identified in simulation \citep{prinz2004similar} and experimentally in live cells \citep{schulz2006variable}.

% In fact it's not just about multiple combinations of conductances to achieve a spiking pattern. There are also multiple ion channels that cover similar voltage-sensitivity space; multiple ion channels that cover similar kinetics space \citep{goaillard2021ion}.

% This is physiological resilience. If the situation changes (extracellular environment, available cell morphology, network activity patterns etc), there's still a space of available ways to succeed. Marder talking about degeneracy: \citep{goaillard2021ion}.

% Biological systems are healthier when they aren't collapsed onto a single solution, but instead have resilience through multiplicity. If one mechanism fails, for example due to changing environmental conditions, others can take over \citep{edelman2001degeneracy, albantakis2024brain}. 

% The boundaries that contextualize collapse here look very different from a cell membrane. What is it that prevents collapse into a single, non-degenerate form? On one level, it's just evolution itself. But what are the more proximate mechanisms? One is lateral inhibition. 

% Lateral inhibition shows up in developmental patterning and in neural activity. Lateral inhibition is *sharpening*: creating an asymmetry. 

% Another form of boundary is that homeostasis targets an activity pattern rather than a particular set of conductances \citep{o2013correlations}. This is what causes diverse underlying solutions to emerge. 

% [we could also say that degeneracy itself is a kind of boundary/contextualization, because it allows the system to maintain integrity across different conditions?]


% % For example, nerve cells create nearly-digital signals using voltage-gated sodium channels. A small initial change in voltage (such as from inputs to the cell) causes some channels to open, which further changes the voltage by allowing in positively charged sodium ions, which in turn opens even more channels. The feedback loop creates an abrupt electrical spike within about a millisecond. 


% Hierarchical structure of organisms. Many somewhat competing drives; nested boundaries to contextualize and hold them in balance.



% \subsection{New form emerges spontaneously when dynamic range is available}

% self-organization. 

% Self-enhancing patterns tend to emerge (cf Blaise's simulations), for example in biological evolution. If these patterns aren't held in check, they will try to convert more and more of the surroundings into their image. Think of cities burned down by fires, cancer, sociopaths, profit-hungry corporations. 

% Crucially: increasing subtlety.

% Alignment is being aware of when any particular thing gets too concentrated, and placing a boundary to protect the ``letting it be" unfolding from that thing. 

% Evolution of evolvability. Evolution keeps discovering mechanisms that make evolution itself work better. This is a form of bootstrapping from myopic to long-sighted optimization. Evolution creates boundaries against short-sighted optimization at all levels \citep{szathmary1995major, grosberg2007evolution, west2015major}. At any given moment, evolution is operating on top of a deep platform of existing boundaries. This is one way of looking at why things spontaneously emerge in such an intelligent way. Parallels to meta-learning.


% \subsection{Collapse}

% At a given scale, health is roughly the opposite of collapse:
% \begin{itemize}
% \item Degeneracy story from systems neuroscience.
% \item Eve Marder story about multiple solutions to achieve the same dynamics. Their possible loss under stress.
% \item The psychosocial resilience story.
% \item The anti-fragility story.
% \item Michael Levin's view of life as multi-scale competency. eg https://arxiv.org/pdf/2201.10346
% \item Agency in the face of many possible futures \citep{shen2025beyond}. 
% \end{itemize}


% Local formalisms are always trying to collapse the world. Any given pattern, as distinct from what is not it, continues to exist because it's successful in modeling the world and thus remaking the world in its own image. This is the most basic definition of natural selection: entities are selected for expanding the scope of their own pattern. This is why there's always a tendency toward extremes and collapse -- which is counteracted by boundaries.

% Collapse means reduction of dimensionality and potential. Fewer transient options for recombining processes to play with. 


% Individuals have a drive toward their own selfish optimization (like overgrazing the common land in tragedy of the commons). If there are no barriers, this optimization goes too fast, and the system collapses. 

% But with barriers, you have everybody trying to optimize for their own thing, but being slowed down: so there's this big space of many unresolved processes co-existing. There's lots of richness to draw on for new potential things happening (new combinations, new solutions etc). 

% When you have lots of unresolved potential floating around, you tend to get interesting new things emerging (self-organization, spontaneous arising, etc). It's not guaranteed, but it's much more likely than in a collapsed system.


% \subsection{Collapse is not bad}

% However, this is not to say collapse is bad. We have collapse in the matter/antimatter asymmetry of the universe; in DNA-based life on Earth; in our conceptual thinking. These all seem pretty ok. Any commitment to any form is a type of collapse. 

% To put it another way, myopic objectives are not an enemy. In fact, myopic objectives are all we have. When boundaries are working well, they don't disable myopic optimization: on the contrary, contextualization harnesses the energy of selfish optimization and puts it to work in a longer-sighted direction. The contextualized process retains agency while also experiencing constraints in relation to other processes -- it is more likely to play a productive rather than dominating role.  

% Boundaries contextualize the tendency toward extremes that collapses diversity. 

% The surrounding system also evolves to contextualize these forces and direct the energy in a productive way: fire is harnessed for cooking; corporations are regulated so that their profit-motive drives innovation and public benefit. The collapse of one animal’s life feeds another’s growth.

% Alignment isn't about preventing all collapse; it's about preventing excess collapse. What excess means is not formally defined and there are no absolute right or wrong answers. Maybe some great extinctions in the past were necessary for the evolution of life; we might not even be able to fully know the answer to this. Even the concept of ``avoiding collapse" is itself gameable and it's helpful to have boundaries against excess attachment to this concept.

% It's not just about putting arbitrary boundaries or limitations. It's inherently subjective what counts as ``too much" in any domain. The boundaries come from the real world. They therefore ground the system in the already-infinitely-rich real world. 


% Clamping something to be nearly formal requires more energy (eg digital computers).




% \subsection{Semi-permeable boundaries}

% An excess of any particular thing is clamping some dimensions all the way to the end of a spectrum, instead of leaving dynamic range in the middle. This reduces subtlety and potential. 

% We ourselves, including our values and principles, will continue to change.  Before these phenomena existed, we may not have even been able to value them, because we lacked the concepts. 

% The boundaries being semi-permeable is crucial. We're not talking about alignment by shutting down all the computers, for example.

% Contexts nest other contexts in an intricate network, not a strict hierarchy.


% \subsection{Boundaries and rate constants}

% Each process, by itself, would converge asymptotically toward an equilibrium. If you took away the rate constants, this would happen instantly and things would collapse. Like, in a forest, wood decays, but it doesn't decay instantly -- in the meantime, the fallen tree is a home for insects. On a long timescale, the sun is burning out, but in the in-between time, we have all this life on Earth. On a shorter timescale, drugs are cleared by the liver, but in the in-between time they have physiologic actions. Rate constants are always acting as barriers for each other: for example, one species evolving a defense against another.

% Barriers create more subtlety/nuance/potentiality because there are more processes that haven't resolved to their asymptotic equilibria. 

% \subsection{Rich boundaries and groundedness}

% Say you have an AI image generator. It makes images that, by self-report, people often fail to distinguish from reality. Yet, underneath, something is missing. The images lack those surprising features [like the examples Steve gave with how a real tree would break]. 

% What the AI has done is to learn to use a bunch of proxies. For example, it learned the pattern of correlation that the eyes should be 2.5 inches from each other. This is just a proxy because the model doesn't understand how eyes develop, why eyes are in front, etc. (Note: A multimodal model would ``know" these things; but then there are deeper embodied things it wouldn't understand, like what certain patterns of gaze-meeting and gaze-avoidance mean, or how to grow an eyeball from an embryo, which we do without any self-report knowledge about it). 

% Or, depending on how you want to look at it, you could say the AI has learned a single proxy for reality/life. The proxy is this big model, with these memorized correlations. 

% As complex as that model is, it's still just a sliver of the complexity of the real world. I.e., it's a shallow proxy. 

% So, you could think of ``alignment" (aka grounding) as the problem of creating boundaries that prevent over-reliance on any particular proxies. any given system with an optimization objective (like the image model following gradient descent under a loss function) is a proxy, and the more you allow it to dominate (for example, if we put more and more compute resources into those models, and more of the images we see on the internet are generated by them), the more we lose connection to the life energy of the real world. we have to create boundaries against over-reliance on that proxy. some examples of boundaries:
% \begin{itemize}
% \item building systems to detect whether an image is real or fake.
% \item creatively thinking of new model architectures to make them understand the world more deeply. this boundary comes from our own groundedness; not being satisfied with something that doesn't feel real.
% \item developing more skepticism about what we see on the internet; having a hunger for real-world interactions instead.
% \end{itemize}

% Because each boundary limits over-reliance on a proxy, boundaries always have the property of ``tapping back into the source" (reality itself, which isn't captured by self-report or other proxies).

% Of course, the ``system with an optimization objective" could be viewed as just the model itself, or it could be viewed as larger loops that include the model as well as the corporations profiting from it, and the people greedily consuming images that satisfy their base desires (even literal AI porn). Analyzing at different levels can give us different intuitions about where we need to create better boundaries.

% `Rich boundaries and groundedness' is similar to `anti-fragility'.



% \subsection{Cycles of heating and cooling}

% Relationship to semi-permeable boundaries. Necessity for cycles in distance.


% Group decision making also illustrates another, more subtle phenomenon. Groups work best when members alternate between thinking independently and combining ideas. This pattern appears in many other settings too: alternating between isolation and exchange \citep{yanai2024takes, hills2015exploration, becker2018wet}. In the isolation phase, diverse solutions have space to specialize without being dominated by others. In the interaction phase, solutions compete on merits or combine into larger structures. The life cycles of organisms are an example: alternating between an elaborated body and a formalized genetic code. Another example is alternation between language and internal representation; or between cycles of training and reasoning in foundation models.

% \section{Boundaries between chemical processes}

% This would be a cool section to add. Deacon has the idea that constraints reciprocally limit processes and give rise to life and consciousness \citep{deacon2012incomplete}. He talks about it mostly at a molecular level.



\section{Acknowledgements} 

Clark Potter for thinking of all this stuff many years ago. Zach Duer for comments on the manuscript.

\section{Competing Interests}

The authors declare no competing interests.

\bibliography{proxyfailure}

\end{document}
