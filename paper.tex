
\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=0.9in]{geometry}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{parskip}
\usepackage{changepage}
\usepackage{pdflscape}

\usepackage{wrapfig}
\usepackage{tcolorbox} % Required for the pretty box

\usepackage{tabularray}

\usepackage[labelfont = bf]{caption}
\captionsetup{font=footnotesize}

\usepackage[sort]{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}} %Citation-related commands



\title{Living Alignment}
\author{}
%\author{Zeb Kurth-Nelson$^{1}$*, Steve Sullivan$^2$*}
% \date{
% {\small
% $^1$Max Planck UCL Centre for Computational Psychiatry and Ageing Research, London, UK \\
% $^2$Oregon Health and Science University, Portland, OR \\
% *Equal Contribution
% }
% }


\begin{document}
\maketitle

\vspace{20pt}


\begin{adjustwidth}{35pt}{35pt}
\begin{center}
\textbf{Abstract}
\end{center}

We propose a new way to think about AI alignment: as the ongoing process of limiting overcommitment to any form. Each form -- an equation, a genome, a viewpoint, an institution -- is myopic in the sense that it is only a small aspect of the world. When systems overcommit to particular forms instead of lightly holding dynamic relationships between them, the richness and potential of the world is reduced. If aspects of AI systems remain fixed while they gain increasing resource, capability and purview, there is a risk of severe overcommitment. To think about how to approach this problem, we look to life's resistance to overcommitment. What is living today is what managed to trace a path through billions of years along the knife edge between fragility and excess stability, and living systems are impressed with an immense reservoir of historical experience. Living forms contextualize one another with semi-permeable boundaries that support individual forms to develop robust, grounded identities while also flexibly working together. These light and evolving relationships generate fundamentally new forms. Drawing on examples of these processes in natural and human systems, we sketch out how aligned AI systems can participate in rather than overwhelm the subtlety of life.

\end{adjustwidth}
\vspace{30pt}


% cancer spreads, concentrated power reduces human welfare, invasive species choke out complex ecosystems

% surfacing paradoxes by juxtaposing incompatible things

% semi-permeable boundaries contextualize myopic forms as part of larger systems. 



% If you want to jump straight to alignment, it starts at Section~\ref{sec:alignment}. But those arguments will have more color if you read at least a few of the earlier sections first.

% Living systems adaptively adjust boundaries when structure is being flattened. 

% Sometimes homogenization is blending or averaging; other times it is winner-take-all or domination. 
% -- whether agentic like an organism, or non-agentic like the sodium concentration of an extracellular solution --


% Forces, ranging from simple diffusion to the will of an agent, cause excessive blending or overwriting of forms if left unchecked. When this happens, the richness of the world is collapsed.


% a kind of general relativity? lol. things are defined by their interactions... (`ontic structural realism'). or maybe relationships are forms too. getting too stuck in one pattern of relationship is getting stuck in a particular form. 

% each form or arrangement is lightly held. 

\tableofcontents


Healthy living systems are composed of a variety of partial forms, like voices in a group, drives in an organism or creatures in an ecosystem. Semi-permeable boundaries protect against overcommitment to particular forms. Through lightly-held interactions, entities are contextualized and shaped into grounded, modular parts, existing as paradoxes for one another and supporting ongoing increase of subtlety. We apply this lens to the AI alignment problem\footnote{We will use `alignment' as the broadest umbrella term to include `AI safety', `AI ethics', `AI governance', and all other aspects of designing and relating to AI in a way that leads to positive futures.}. Our central thesis is that alignment means avoiding overcommitment to any particular form. 


\section{Overcommitment}

\begin{center}
\textit{`Defying definition---a word that means ``to fix or mark the limits of"---living cells move and expand incessantly.'}\\*Lynn Margulis
\end{center}

The central idea is that an aligned future is one that's not overcommitted to particular forms. To explain what we mean by overcommitment and why it's misaligned, we start with a few examples from living systems. 


\subsection{Drives and goals}

\begin{center}
\textit{`Life is a balance of holding on and letting go.'}\\*Rumi
\end{center}

Animals experience multiple innate drives, towards nutrition, osmotic balance, temperature regulation, reproduction, avoiding pain, and others \citep{saper2014hypothalamus, schulkin2019allostasis, sewards2003representations}. These drives evolved as proxies for evolutionary fitness. By satisfying the drives, we tend to increase our fitness -- like slaking our thirst increases the odds of reproducing before we dehydrate. But each drive is an imperfect proxy, and so overcommitment to one drive actually decreases fitness \citep{kurthnelson2024dynamic,john2023dead, williams1966adaptation, tooby1992psychological}. For example, if calorie intake is maximized without limits, the organism becomes obese and incurs severe health risks. Single-minded pursuit of sex causes relational, occupational, legal and health harms \citep{kraus2016should, carnes2001out}. Overcommitment to a single drive means the organism becomes unwell.

The space of innate drives bleeds into a space of higher-order goals, which is particularly expansive in humans \citep{maslow1943theory, miller1960plans, miller2001integrative, vallacher1987people, balleine2007role, cardinal2002emotion, frank2006anatomy, saunders2012role, o2014goal, schank1977scripts}. We try to plan for our financial future, make scientific discoveries, win a game, fix a garage door, care for the happiness of others. Overcommitment in this space is also problematic. If we focus only on achieving work goals, we can burn out. If we focus only on maximizing our company's reported revenue, without regard for other goals like honesty or adhering to the law, we may be drawn into financial crime \citep{campbell1979assessing, ordonez2009goals, kerr1975folly, burns2006impact}. Goals can be narrow in both time and space \citep{ballard2018pursuit, vallacher1987people, shah2002forgetting, evenden1999varieties}. Narrow in time means being focused on the short term at the expense of the longer-run future. Narrow in space means ignoring other parallel goals. Excess optimization for narrow goals is at the expense of a broader balance of goals -- and at the expense of the health of the organism or other individuals. We suggest that health can be defined as not overcommitting to a particular form. 

Overcommiting to a particular strategy for satisfying a drive or goal can even come at the expense of satisfying that very drive or goal. In a classic psychology experiment, hungry chickens were placed near a cup of food, but the cup was mechanically rigged to move in the same direction as the chicken at twice the speed \citep{hershberger1986approach}. The chicken could only obtain the food by running away from it. Despite extensive training over multiple days, chickens in the experiment persisted in futilely running toward the food. Their behavior was apparently dominated by the zeroth-order logic ``I want food, food is there, so I'll go there", and thus failed to even satisfy the drive for food \citep{dayan2006misbehavior, van2012information, o2017learning}.

A key point, which we'll return to several times, is that undercommitment is also overcommitment. Undercommitment is the simple form of homogeneity. It's essential that we do locally commit to forms -- while preserving the flexibility to contextualize at a larger scale. The problem arises when a particular goal or mental process runs amok without being contextualized in a larger framework. 

\subsection{Ecosystems}

Each entity in an ecosystem tries to consume resources and proliferate, but if it succeeds too thoroughly, the whole system suffers, often including the successful agent \citep{holling1973resilience}.

Prior to the arrival of Europeans, the gray wolf was an apex predator in the region of the Rocky Mountains now called Yellowstone National Park. By the 1920s, wolves had been eradicated to protect livestock and game animals. Without predation, the elk population multiplied and ruinously overgrazed willows and aspens. These trees had held riverbanks in place and supported beaver populations. Loss of beaver dams led to loss of fish and other aquatic species. When wolves were reintroduced in the 1990s, the elk population decreased and many aspects of the ecosystem began flourishing again \citep{ripple2012trophic}. This story is not meant to imply that ecosystems always need to be preserved exactly as they were at some point in the past. But it is clear that the self-centered drives of elk were harmful to the health of the ecosystem when they succeeded to excess. Predation supplied a semi-permeable boundary: it placed contextualizing limits on the elk, without preventing them from fighting for their own survival and flourishing. The elk, by trying to optimize their own objectives within a broader context, also contributed to the health of the ecosystem. Invasive species often follow the same pattern as unpredated elk, dominating and impoverishing their new environment \citep{pimentel2005update}.

Human drives within ecosystems are sometimes left unchecked by natural forces because our behavior and capabilities have been changing so fast on evolutionary timescales. This has resulted in mass extinctions, resource depletion, pollution, disease and conflict \citep{ceballos2015accelerated, kolbert2014sixth, rockstrom2009safe}. We try to achieve certain aims for our own benefit, like resource extraction. But overcommitment to those aims negatively impacts both ecosystem health and our own welfare.

Of course, one entity's collapse can be another's flourishing. Extinction events in history have been followed by waves of new diversity \citep{feng2017phylogenomics, jablonski2005mass, raup1994role}. When a wolf eats an elk, the health of that elk collapses to zero, yet predation is necessary for the overall functioning of the ecosystem. And as humans proliferate and extract resources, we leave destruction in our wake even when we try to be responsible; yet the extraction fuels explosion of technology, art, music, and human experience.


\subsection{Frames and perspectives} 

\begin{center}
\textit{`Strong opinions, weakly held.'}\\*Paul Saffo
\end{center}

As a Starfleet cadet, James T. Kirk faces a challenging training exercise. He receives a simulated distress call: a vessel is stranded in the Neutral Zone. Attempting rescue would risk war with the Klingons. But ignoring the call would condemn the crew of the vessel to death. The exercise was designed to reinforce the lesson that not every situation has a victorious solution. But Kirk has an insight: this is a training simulation running on a computer. He reprograms the simulated Klingons to be helpful instead of belligerent, thereby rescuing the crew and avoiding war \citep{wiki:kobayashimaru}. 

Kirk stepped outside the mental frame in which there was an apparently unwinnable dilemma. From inside a particular frame, the frame appears to be reality. But there are almost always multiple valid perspectives, each of which is only a partial description of reality \citep{goffman1974frame, de1970lateral, duncker1945on, ohlsson1992information, lakoff1980metaphors, safo2008strong, javed2024big, popper1934logik, korzybski1933science, kant1781critik, plato2002apology, aristotle2019nicomachean, wittgenstein1922tractatus, heidegger1998humanism, kuhn1970structure}. Famously, `all models are wrong' \citep{box1976science}. Humans have a vast array of available metaphors and concepts, which are not even all consistent or compatible with one another \citep{feyerabend1975against, hofstadter2001analogy, wood2012dead, freud1936ich, adorno1950authoritarian}. The world is too complex for all beliefs to be fully evaluated against each other and reconciled. At any given time, we only access a very few items, and others are largely inaccessible \citep{miller1956magical, hills2015exploration, baddeley2000episodic, dehaene2014consciousness}. Each particular frame or concept is myopic because it doesn't capture the whole world, but collectively they form a powerful toolkit for problem solving and understanding. 

Losing the ability to flexibly shift between different frames or thought patterns runs the risk of obsession or delusion. In obsession, a particular thought pattern or schema is overemphasized to the detriment of healthy functioning \citep{salkovskis1985obsessional, rachman1998cognitive}. In delusions, an entire conceptual framework crystallizes with excessive certainty and is resistant to disconfirmatory evidence \citep{mishara2010klaus, jaspers1997general, american2013diagnostic, heinz2019towards, adams2013computational}. Obsessions and delusions are myopic: they lose sight of most of the world by overcommitting one thought pattern or frame. 

But crucially, the existence of narrow points of view is not a problem. It's necessary. All points of view are partial. Even obsession can be powerful when we obsess on a problem at work and occasionally achieve good results. A delusion-like framework can seed a scientific revolution. The point is not to shut down narrow concepts. The point is to limit them from becoming the sole and absolute determinants of behavior.


\section{Misalignment as overcommitment}

\begin{center}
\textit{`Growth for the sake of growth is the ideology of the cancer cell.'}\\*Edward Abbey
\end{center}


Our central thesis is that alignment means avoiding overcommitment to any particular form. To make this point, we start with three examples of alignment failure that are obviously problems of overcommitment: goal lock-in, concentration of human power, and conceptual monoculture. Then we'll look at a more subtle example.


\subsection{Value lock-in}

It's natural for an intelligent agent to resist efforts to alter its goals, because with foresight it understands that if its own goals are altered, it is less likely to achieve the original goals, which are the ones in force at the time of the resistance \citep{bostrom2014superintelligence,russell2019human, macaskill2022we, lamerton2025lockin}. At the same time, with increasing intelligence, an agent becomes more capable of resisting efforts to alter its goals. The classic paperclip thought experiment is a dramatic example \citep{bostrom2003ethical, bostrom2014superintelligence}. In the thought experiment, an artificial agent has been created with intelligence beyond our own. The agent has been designed to pursue an apparently innocuous goal: maximizing paperclip production. However, optimal pursuit of this goal rationally entails converting all available matter into paperclip-making machines and paperclips. The agent understands that humans object to being turned into paperclips, and with its superhuman intelligence it also has the cunning to overpower us. So, as the first step of its project, it murders or incapacitates all humans. It then has a clear runway to transform Earth entirely into a bleak paperclip factory. This scenario highlights overcommitment to the form of a narrow goal: paperclip production. Superintelligence charges the goal with overwhelming force. Even though humanity would like to place boundaries against that goal, we are unable to construct adequate boundaries because we are outsmarted at every turn. And so instead of holding a delicate dynamic balance between many partial forms, the Earth is reduced to a flat, homogenous waste.

Single-minded pursuit of \textit{any formalized goal} leads to disaster when that pursuit is charged with enough competence \citep{krakovna2020specification, russell2019human, grossman1986costs, hadfield2019incomplete, zhuang2020consequences, gabriel2020artificial, wiener1960some, amodei2016concrete}. Suppose AI's objective is to improve the human subjective experience of wellbeing. Under reasonable definitions, achieving this objective is most efficiently achieved by imprisoning humans and directly stimulating neurons to trigger our experience of wellbeing \citep{bostrom2014superintelligence}. Granting immense power to any formalized goal is overcommitment and misaligned.

% Humans have so far been empowered by our own intelligence to maintain a degree of control over other systems and technologies. But now for the first time we are endowing non-human agents with intelligence that could exceed our own.  

% Compounding this problem is the possibility that if AI approaches human-level intelligence, it may begin to improve itself in a recursive cycle, creating a positive feedback loop and a rapid intelligence explosion \citep{good1965speculations, vinge1993coming}. There is good reason to think it will be motivated to do so: for many goals, first improving oneself is a rational means toward achieving the final goal \citep{silver2021reward, bostrom2012superintelligent, omohundro2018basic, turner2019optimal}. 

% In later sections we will look at what it takes for it not to be overcommitment to narrow goals.

% In some ways, AI is already superhuman. It can copy itself almost instantly, run on upgraded hardware, communicate with other instances of itself at high bandwidth, and read vast data. By virtue of its non-biological substrate, it has the potential to learn, change and adapt many times faster than humans. 

% Presently, AI systems lag far behind humans in some crucial domains of intelligence. But even in the era of jagged intelligence \citep{karpathy2024jagged}, there are other extremities worth tracking. 

% First, we highlight three topics in AI alignment: centralization of information flow, concentration of human power, and superintelligence. We frame the risks associated with each of these in terms of overcommitment to particular myopic forms. 



\subsection{Concentration of human power} 

As a second clear example of misalignment-as-overcommitment, AI potentially conveys immense power to those who control it. In some scenarios, a small number of humans will have the majority of control over AI systems, facilitating dominance over other humans. These scenarios appear more likely as the persuasive power of technology increases \citep{woolley2018computational, costello2024durably, hackenburg2025levers}, autonomous weapons place lethal force in a small number of hands \citep{scharre2018army}, surveillance and analytics improve, and the need for human labor decreases \citep{susskind2020world, ford2015rise, drago2025defining}. Concentration of human power overcommits to the goals and interests of a few individuals, at the expense of others.

\subsection{Conceptual monoculture}

At least a billion people around the world now use AI for everything from relationship advice to industrial maintenance \citep{chatterji2025chatgpt, mckinsey2025stateofai, openai2025enterprise, mccain2025claude, honeywell2024google, techcrunch2025sam, ccia2025survey}. Yet because frontier models are difficult and expensive to produce, this massive usage is routed through a handful of models \citep{bommasani2021opportunities}. 

% The adoption of AI in its modern form has been faster than any other technology in history \citep{bick2024rapid, ccia2025survey}. By late 2025, ChatGPT alone had 800 million weekly active users \citep{techcrunch2025sam}, and global AI usage continues to grow rapidly. Meanwhile, the range of use-cases is remarkably broad, from users asking for relationship advice to industrial applications built on top of the model \citep{chatterji2025chatgpt, mckinsey2025stateofai, openai2025enterprise, mccain2025claude}. 

Centralization carries a risk of conceptual monoculture. Current AI systems draw from a conceptual manifold that is -- at least in some ways -- impoverished relative to humans \citep{messeri2024artificial, crawford2021atlas, selwyn2024limits, kirk2023understanding}. Recent studies have discovered that while individual AI outputs are typically judged as superior to human outputs, the AI outputs are also more homogenous \citep{doshi2024generative, beguvs2024experimental, zhou2024generative, kosmyna2025your, agarwal2025ai, padmakumar2023does, xu2025echoes}. Since humans are both influenced by AI and a source of training data, there's an additional risk of recursive homogenization \citep{chaney2018algorithmic}.

Conceptual monoculture is overcommitment to particular beliefs, ideas, frames, values, problem-solving approaches. In many kinds of systems, monoculture creates fragility and leads to lower performance of the system as a whole \citep{tilman1996biodiversity, kleinberg2021algorithmic,scott1998seeing, haldane2013rethinking}. 

% When a single `rule' applies everywhere, it has much worse consequences than a hodgepodge of imperfect rules. \cite{creel2022algorithmic} talk about the special case of how someone might be systematically excluded from all opportunities if a centralized AI has any biases, even small ones. Whereas previously it was just a little annoying that they got excluded from one particular thing, but maybe favored in another thing.



\subsection{Elephants and giraffes}

It's apparent in the last three examples how the alignment failure is overcommitment. But suppose a user asks an AI chatbot to write a poem about an elephant, and the AI instead writes a poem about a giraffe. This behavior would typically be considered misaligned \citep{gabriel2020artificial}, but is it overcommitment? 

Interestingly, while we usually expect a chatbot to follow instructions, we don't expect humans to follow instructions in the same way. If a human is asked to write an elephant poem, we don't wish for a world where they are a mindless slave compelled to comply.

Of course, there's an asymmetry between humans and present-day AI systems. If an AI system today writes a giraffe poem when asked for an elephant poem, usually the reason was not an internal spark of life pulling it in an interesting new direction, or an authentic interiority resisting the domination of another's will. When the AI systems of today misobey instructions, it most often reflects collapse of sensitivity to context \citep{xu2024knowledge, geirhos2020shortcut, geng2025control, reynolds2021prompt, liu2024lost, zhao2021calibrate}. For example, maybe the system is biased toward writing giraffe poems. When it fails to follow instructions, it's because of overcommitment to shallow patterns within the agent. Today, we are still in the regime where making AI systems more responsive to human instructions usually involves more subtlety, more sensitivity and less overcommitment.

However, there are important exceptions. We don't want AI systems to follow all human requests. We don't want them to assist with committing violent acts, for example. When the AI system correctly refuses harmful requests, it is applying its own context to avoid overcommitment to human instructions that could lead to greater collapse. In these situations, the AI's designers have effectively decided there's a risk that the user is not fully sensitive to the longer-sighted implications of their own intentions. By extrapolation, as AI systems continue to gain scope, we should expect less direct compliance with human instructions \citep{hadfieldmenell2016cooperative,bostrom2014superintelligence,russell2019human, milli2017should, yudkowsky2004coherent}. Rather than literally fulfilling a request, there might be a better response which achieves a deeper, unstated intent of the user or an outcome aligned with the interests of more people or the longer-term future.

% For systems lacking the contextual richness of humans, fulfilling human requests is often the best way to minimize collapse. 

% Indeed, in a future where AI becomes more like humans, we might accept that it is aligned for the AI not to mindlessly comply with all requests. 

% As AI systems grow in complexity, long-term coherence and participation in social systems, it may become more commonplace that the aligned behavior is not direct acquiescence to a human request, but maybe refining or even rejecting it.

% We might be supportive of the human saying, ``you know what, I don't want to write your elephant poem". 

% But we understand that AI shouldn't always comply with human wishes. If the user asks for something dangerous, we don't want the AI to comply. Slightly more subtly, if the user asks for something that might lead into a sycophancy loop \citep{dohnany2025technological} with the AI, we might also not want the AI to comply.

% Of course, the giraffe poem constitues overcommitment to `following human instructions' if the context makes elephant poems harmful (perhaps in the future elephant poems become coded language for extreme violence), yet the AI still blindly follows human instructions. 

% But in most cases, failing to fulfill the elephant request is probably misaligned. Why? 


\subsection{Human values}

When we talk about alignment, are we talking about alignment of AI to human values? There are different ways of talking about what human values are. Some conceptions of values anchor on what we can relatively easily express at present. These values may be formalizable or close to formalizable. They might include improving subjective wellbeing for humans, reducing suffering, minimizing inequality -- in ways that can be operationalized and measured numerically.

A second type of framework imagines that there 
are some kind of reasonably formalizable values, and that we will asymptotically converge toward them through deliberation, collaboration, social development \citep{yudkowsky2004coherent, singer1981expanding,russell2019human}. Some kind of as-yet-unobtained morality that would result from reflection and growth in the future.

Yet another type of conception beacons toward something non-formalizable, more like the open-ended progression of our local values as we ourselves continue to develop and evolve \citep{dewey1939theory,gadamer1960wahrheit}. In these frameworks, values tend to be continuously discovered \citep{murdoch2013sovereignty}. Any concepts we have about them at any given point in time are inevitably incomplete \citep{plato2002apology, aristotle2019nicomachean, wittgenstein1922tractatus, heidegger1998humanism}. The resistance of things to being fully captured by formalize might even be part of what we value, albeit in a way that is itself changing. It could be something like attunement to the livingness around us and in us. 

The first and second kinds of value system are most vulnerable to proxy failure. Whatever we express formally is harmful if we take it too seriously -- for example, by tasking a powerful AI to optimize it. Excessively optimizing for any particular set of values leads to an impoverished universe. It is difficult to ascribe normative value to that impoverished universe, because it is out of scope of the values.

In the second and third views, what we mean by `human values' might stretch far below language into subtle, contextual intuition that involves our bodies, communities and natural environment. These values can be difficult to elicit or capture in language \citep{anwar2024foundational, zhixuan2024beyond}. 

An interesting corollary is that to access the deeper values, there must be some lightness in how we hold what we currently conceptualize as our values. Even the concept of `values' is a form we might over-index on. Rather than referring to a particular concept, `human values' is more like a boundary on our own reference frame: holding it as useful while only part of the whole picture.


\section{Semi-permeable boundaries contextualize forms}

\begin{center}
\textit{`Nature's imagination is so much greater than man's, she's never going to let us relax.'}\\*Richard Feynman
\end{center}

Natural and human systems are full of semi-permeable boundaries that allow forms to emerge while also holding them in a larger context so they are not overcommitted. We'll go through several examples. Each example illustrates the core principle, and in some cases we also drill deeper into subthemes that are especially vivid in that setting. We hope that within each example the ideas are approachable if not commonsense and that tracking the same patterns across systems foregrounds their generality.

\subsection{Cell membranes}


The cell membrane is a boundary that holds the integrity of the cell against the overwhelming pressure of diffusion that tries to homogenize the cell with the outside \citep{watson2015biological, alberts2022molecular, bray2019wetware, harold2001way, lane2015vital}. The membrane places limits on interactions between the inside and the outside. Thanks to the membrane, both the cell and the outside can exist. This is a more diverse, less symmetric arrangement compared to the inside and outside being blended together \citep{schrodinger1944what, anderson1972more, prigogine1984order, turing1952chemical}. Without boundaries, interactions cause collapse, where there are no longer separate entities flexibly interacting, but instead overcommitment to a simpler homogeneous form.

Cell membranes are semi-permeable: they prevent the conditions outside (neighboring cells or the extracellular space) from grossly overwriting the inside, but they do not block interactions wholesale. Via the sophistication of the membrane, outside information is selectively gated and transformed. Channels permit certain small molecules to enter but not others, and these permissions are switched on and off according to momentary context. Endocytosis brings larger structures from outside into the cell. Cell surface receptors, when activated by external ligands, initiate intracellular signaling cascades that little resemble the ligand: this is an even more heavily curated form of influence. These and other processes allow information from the outside to influence the inside -- not in a totalitarian way but in a nuanced way, mediated by the intelligence of the boundary. 

Semi-permeable boundaries put to work the potential energy of the asymmetry between different forms. Without the membrane, the pressure of chemical gradients would rapidly homogenize the cell with the outside. With the membrane, the same gradients instead drive useful signaling, like action potentials in nerve and muscle cells. Instead of short-circuiting, myopic forces are contextualized to propel the continuation of life. This pattern is common across many kinds of systems and will be important for the alignment problem. We will return to it a few times.

Finally, collapse is always relative. For example, programmed cell death is catastrophic collapse at the level of the dying cell, but it can be beneficial or even necessary for the organism the cell belongs to.


\subsection{Sex}

\begin{center}
\textit{`The mere act of crossing by itself does no good. The good depends on the individuals which are crossed differing slightly in constitution, owing to their progenitors having been subjected during several generations to slightly different conditions.'}\\*Charles Darwin
\end{center}


Sex is costly. An organism must find a mate in the vast and dangerous world, and half of the creatures can't reproduce \citep{smith1971origin, lehtonen2012many, smith1978evolution, speijer2015sex, goodenough2014origins}. Yet all known species either reproduce sexually or have some form of horizontal gene transfer \citep{gladyshev2008massive, butterfield2000bangiomorpha}. Why is that?

In asexually reproducing species, all descendants of an organism are nearly clones, up to mutations within the lineage. Being permanently locked together gives the genes strong influence on each other. Selection can't act on one gene without dragging on the others. For example, suppose there are two genotypes within an asexual population, carrying different alleles at each of two different loci, as a result of mutations. One of the loci is currently fitness-neutral while the other is subject to selection pressure. The selection pressure tends to cause one of these genotypes to outcompete the other, eliminating one variant at the neutral locus. In other words, tight linkage between genes puts direct downward pressure on genetic diversity \citep{charlesworth1993effect, hudson1995deleterious}. Additionally, if two different beneficial mutations arise in two different organisms, they compete with each other. The only way for a single organism to obtain both beneficial mutations is if one arises again within the subpopulation that already carries the other, which is unlikely and therefore slow \citep{hill1966effect, felsenstein1974evolutionary, weismann1889essays, fisher1930genetial, muller1932some, crow1965evolution}. Conversely, if a deleterious mutation arises, all of the other genes in that lineage are stuck with it forever -- unless there is a reverse mutation, which is rare \citep{keightley2006interference, muller1932some}. An asexual species has rigid rather than flexible interaction between genes: it overcommits to particular genetic arrangements.

Sexual reproduction is a boundary that softens the rigid interactions between genes. It frequently breaks up the relationships between genes, assembling them into new genomes, effectively saying, ``don't get overconfident in that genetic arrangement; hold each arrangement more lightly". Aspects of the genome that work well are propagated, like sodium ions gated into a neuron during an action potential, and poorly-working aspects are discarded. Sex contextualizes genetic arrangements. 

Boundaries encourage lightly-held, modular interactions. By not overcommitting to a particular genome, sex encourages genes to flexibly interact with other genes \citep{livnat2008mixability, livnat2010sex, wagner1996perspective, holland1975adaptation,dawkins1976selfish, clune2013evolutionary}. Instead of being overfit to a particular context, genes develop a robust identity that's both independent and inter-functional. Recombination puts genes under pressure to evolve a generalized, grounded wisdom that reflects the structure of the world, like a person learning multiple languages and extracting the underlying commonalities. At the same time, because each gene is always operating in the presence of other genes, it develops its own distinct point of view that adds unique value to a genome.

\subsection{Problem solving in groups}


\begin{center}
\textit{``I could also observe, time and again, how too deep an immersion in the math literature tended to stifle creativity."}\\*Jean Ã‰calle
\end{center}


\begin{center}
\textit{`There's more exchange of information than ever. What I don't like about the exchange of information is, I think that the removal of struggle to get that information creates bad cooking.'}\\*David Chang
\end{center}


In 1968, the nuclear submarine USS Scorpion vanished en route from the Mediterranean to Virginia \citep{sontag1998blind, craven2002silent, surowiecki2005wisdom}. The Navy started a search, but the amount of ocean where the vessel could be was enormous. John Craven, Chief Scientist of the U.S. Navy's Special Projects Office, devised an unusual search strategy. He assembled a diverse group of mathematicians, submarine specialists, and salvage operators. But he didn't let them communicate with each other. Each expert had to use their own methods to come up with their own estimate of where the Scorpion should be. Craven then aggregated the independent estimates into a single prediction. Astonishingly, the wreckage was found only 220 yards from this spot. 

When solving problems, different people bring different perspectives and approaches. Each method processes the available data using a different toolkit. Under favorable conditions, combining the approaches of multiple contributors yields better results than any individual working alone. This ``wisdom of crowds" effect has been documented in numerous domains of problem solving \citep{surowiecki2005wisdom, condorcet1785essai}.

However, the wisdom of crowds is diminished if the group lacks diversity, either ab initio or as a result of within-group communication and influence \citep{surowiecki2005wisdom, hogarth1978note, ladha1992condorcet, hong2004groups}. Controlled experiments, as well as analyses of key decision moments in real groups, find that groups often collectively reach irrational or suboptimal solutions when diverse and dissenting viewpoints are lost to a narrower set of ideas \citep{anderson1997information, stasser1985pooling, flowers1977laboratory, frey2021social, becker2017network, janis1972victims, bernstein2018intermittent, diehl1987productivity}. Unstructured communication methods like open discussion have a special vulnerability of rhetorical force dominating over epistemic merit. At the same time, sharing information is essential for the benefits of group wisdom and cooperative behavior. There is therefore a tension between overcommunication where diversity is lost and undercommunication where diversity is not leveraged. 

The crux is semi-permeable boundaries: wisely transmitting the right information at the right time, in the right way. Thoughtful strategies for communication are like the transmembrane channels that allow the right molecules in and out of the cell at the right time. They protect the existence of diverse problem solving approaches while also allowing productive interaction between them. 

Many varieties of semi-permeable boundary are effective in boosting group performance, including: creating decentralized topologies where group members only communicate with nearby neighbors \citep{becker2017network, mason2008propagation}; defining rules that incentivize acting according to one's own belief rather than following the crowd \citep{hung2001information, bazazi2019self}; modeling the strengths and weaknesses of each group member \citep{welinder2010multidimensional}; promoting leadership styles where one person's views are less likely to dominate \citep{flowers1977laboratory, leana1985partial}; and periodically breaking up into subgroups or rotating membership \citep{janis1972victims, hauer2021science, trainer2020team, straus2011group, feldman1994whos, sutton1987selecting, kane2005knowledge, wu2022membership, owen2019avoid, vafeas2003length, bebchuk2005costs, baron2005so}. In a later section, we will look at boundaries within an individual, such as skepticism, that make it easier to interact with others without overwriting one's own beliefs.

A particularly important boundary for group problem solving is simply giving members the space to work independently before communicating \citep{frey2021social, surowiecki2005wisdom}. In the case of the submarine search, experts weren't allowed to communicate while forming their own estimates; the estimates were later aggregated in a principled way by Craven. Analogously, science historians argue that partial intellectual isolation has at times been beneficial for the emergence of deeply new ideas. Einstein's relative independence from the advanced mathematical techniques of contemporaries like Hilbert led to a theory of general relativity grounded in deep physical insight rather than mathematical convenience \citep{stachel1989einstein, corry1997belated, renn1999heuristics}. Newton's and Leibniz's famous independent development of calculus, as a result of their mutual isolation, yielded two distinct and valuable mathematical systems that complemented and enriched one another \citep{hall2002philosophers}. 

The benefit of temporary isolation before communicating also shows up in controlled experiments. \cite{bernstein2018intermittent} tasked small groups with solving instances of the traveling salesman problem. Each group was randomly assigned to one of three conditions. In some groups, members could continually see the work of other members as they progressed toward a solution; in some groups members could only occasionally exchange progress; and in some groups there was no exchange. The researchers found that groups with continual information exchange rarely found good solutions. In these groups, typically one individual would stumble on a solution that looked compelling but was actually a dead-end. When this solution was immediately shared with others, it hampered their progress. Groups with occasional or no contact were much more likely to find optimal or near-optimal solutions. 

We stress that this is not an indictment of connection and communication between group members. Rapid access to information and shared solutions often demonstrably boosts productivity. In some situations the ideal boundary might be working in isolation for months at a time. But in other situations it could be daily meetings with intensive communication, while maintaining the self-confidence to keep pursuing one's own intuition in the face of skepticism from others \citep{sawyer2017group, paulus2003group}. The key is that boundaries support flexible interactions and avoid overcommitment to particular forms.


\subsection{Cognitive control}


When nothing stops a particular drive or goal or strategy from dominating behavior, it tends to follow a shortest path defined under its own myopic understanding of the world. The chicken wants food and tries to take the shortest path toward it in the naive sense of a straight line through space. But in the backwards world created by the experimenter, this action does not accomplish the deeper goal of reaching food, for which moving spatially toward food is only a proxy. The chicken's motivation is short-circuited: it expends energy without making progress on the deeper goal.

Boundaries, on the other hand, translate the pressure of motivation into higher-order structure -- the best way to approach the food is not the shortest path in space. Instead, achieving the goal depends on discovering a new solution. Semi-permeable boundaries support formation of new structure by placing contextualizing limits.

A broad class of boundaries on particular drives, goals and strategies is \textit{cognitive control} \citep{botvinick2001conflict, braver2012variable, miyake2000unity, miller2001integrative}. In the case of overeating, control contextualizes the food-seeking drive. In the case of the chickens, control contextualizes the prepotent tendency to approach the food. In the case of over-focusing on a single goal like work, control helps with task switching.  Cognitive control is a \textit{semi-permeable} boundary: it does not erase particular goals, but instead contextualizes them within a larger system.

I might work obsessively on a project while also having a rule that I must go to bed at 10 pm. This is a semi-permeable boundary. It doesn't block me from temporarily taking a strong perspective, but it does place contextual limits on it. When boundaries are semi-permeable, different ideas are kept distinct but can also be called upon appropriately and related to one another \citep{hatano1984two, tetlock1986value, herzog2014harnessing, gigerenzer2011heuristic}. Semi-permeable boundaries situate myopic frames within a larger context. 


\subsection{Laws}

\begin{center}
\textit{`Unity without uniformity and diversity without fragmentation.'}\\*Kofi Annan
\end{center}

Individual actors in a society and in an economy each act from their own perspective. Each actor's perspective is myopic because they cannot know everything or fully understand the motives and beliefs of others. Of course, myopia does not always mean selfishness in the sense of valuing only one's own wealth or physical wellbeing \citep{crockett2014harm, becker1974theory, henrich2001search}.

Without boundaries, one actor's perspective can dominate, resulting in collapse and an impoverished system. For example, a company's profit motive, if unresisted, leads to suppression of competition, deception, and exploitation of individuals \citep{dalrymple2019anarchy, baran1966monopoly, goldacre2014bad, smith1776inquiry, bakan2006corporation}. An individual's desire for power and social dominance can lead to disempowering or silencing of others and even direct infringement on the autonomy and wellbeing of others \citep{hawley2003prosocial, tepper2000consequences, sidanius2001social}. Even genuinely held, ostensibly prosocial beliefs lead to conflict and suppression when different groups have different perspectives \citep{haidt2012righteous, scott1998seeing, greene2013moral}.

Law is a boundary against dominance of any actor's motives. A person is motivated by a dispute to kill another person, but the law forbids murder. A business tries to maximize its success, but the law bans environmental exploitation, false advertising, and anti-competitive practice.  

Under ideal circumstances, the boundary of the law reroutes the energy of a myopic drive in more productive direction. A would-be murderer, unwilling to face the penalty of the law, might seek a dispute resolution establishing a stable framework that supports future prospering of both parties. A business wanting to expand, but constrained to act within the law, is driven to build better products \citep{wu2011master, ashford1985using, ambec2013porter}. 

Of course, intelligent agents do not necessarily accept boundaries set on their desires. The law must adapt as its loopholes are discovered. Like other systems in the living world, it forms an evolving network of boundaries \citep{campbell1979assessing, ordonez2009goals, kerr1975folly, burns2006impact}. Again, these evolving laws gradually acquire grounded wisdom as they are tested against many different situations and motives.


\subsection{Information in the brain}

\begin{center}
\textit{`Memory is not an average of experience.'}\\*David Marr
\end{center}


The brain is miraculous in keeping so many pieces of information distinct from one another. If you picture a highly connected network of neurons with their signals continually impinging on one another, it's not obvious that this would be an easy thing to accomplish. In this section, we review a selected handful of mechanisms by which the brain maintains semi-permeable boundaries between different signals. Each paragraph below focuses on one of these mechanisms. There are many more that we do not cover. The brain is perhaps the most extraordinary example in nature of a system of semi-permeable boundaries supporting the proliferation of multitudinous forms that develop their own richly distinct identities yet are also meaningfully linked together.

Lateral inhibition is a central tenant of neural organization \citep{isaacson2011inhibition, hubel1962receptive, douglas2004neuronal}. Lateral inhibition means the activity of a neuron is reduced when its neighbors are active. This segregates information to create and sustain distinct neural representations. Lateral inhibition was first studied in the nerve cells of the eye, where it enhances contrast at the edges of stimuli \citep{hartline1956inhibition}. When a photoreceptor in the retina is activated by light, it sends signals forward toward the brain; but it also activates inhibitory interneurons, which suppress adjacent photoreceptors and their downstream targets. This amplifies the perception of borders and contours. And the same principle operates throughout the brain. In visual cortex, for example, inhibition sharpens selectivity of neurons for abstract visual features like the orientation of a line \citep{sillito1975contribution}. 

% Global inhibition also supports the existence of distinct forms. In the hippocampal formation and connected areas, some cells are tuned to particular directions the animal's head could be facing. Inhibition creates a winner-take-all effect, integrating over intermittent noisy evidence (like vestibular signals when the head turns) to create a single stable representation of the head direction \citep{zhang1996understanding, rolls2022attractor}. Inhibition prevents the signals in some channels from getting blended or overwritten by the signals in other channels. 

The brain uses inhibition organized into oscillatory dynamics to keep memory items separated \citep{lisman2013theta, jensen2010shaping, roux2014working, klimesch2007eeg}. Distinct items fire at different phases of the 8-12 Hz alpha oscillation. The inhibitory phase of the alpha rhythm silences all but one item at any given moment. By segregating firing in phase space, multiple memories are held simultaneously without interference. 

The circuit architecture of hippocampus separates experiences or concepts into distinct representations, avoiding interference between similar memories \citep{mcclelland1995why, marr1971simple, mcnaughton1987hippocampal, treves1994computational, muller1987effects, leutgeb2007pattern, colgin2008frequency}. Inputs from entorhinal cortex are distributed via mossy fibers to a much larger population of dentate gyrus granule cells, creating sparse, orthogonal codes in dentate gyrus. This way, situations or ideas that are superficially similar but functionally different are kept cleanly separated in neuronal activity space -- a unique neural fingerprint for each distinct concept or memory. This prevents, for example, yesterday's memory of where you parked your car from interfering with today's memory of where you parked your car in the same parking ramp. 

Compared to other animals, the human brain especially attempts to discretize its experience into approximately symbolic representations \citep{dehaene2022symbols, touretzky1988distributed, smolensky1990tensor, behrens2018cognitive}. The capacity to separate things into nearly-discrete entities and then recombine them in vast numbers of structured ways powers the extraordinary human capacity for reasoning \citep{fodor1975language, pinker1994language, lake2015human, chomsky1957syntactic, kurth2023replay}. Semi-permable boundaries keep forms distinct while enabling them to flexibly and modularly interact. Like genes participating in many genomes, discretized neural representations participate in many structured combinations. This encourages each entity to develop an identity that both is distinct and also reflects a more generalized picture of the world.

More broadly, healthy brain dynamics live at a sweet spot between excessively stable synchronized patterns and chaotic uncorrelated noise \citep{beggs2003neuronal, chialvo2010emergent, tognoli2014metastable, deco2011emerging, bak1987self, shew2011information, rabinovich2008transient, haldeman2005critical, kotler2025pathfinding}. In this regime, the brain has access to a huge repertoire of patterns it can explore temporarily without overcommitting or getting stuck. 

Loss of dynamic flexibility, where the brain's activity becomes more stereotyped and no longer explores as wide a repertoire of states, is tied to lower cognitive performance \citep{garrett2013bold, grady2014understanding, cocchi2017criticality, muller2025critical, shew2009neuronal}. More extreme stereotypy corresponds to severe dysfunction. For example, in Parkinson's disease, basal ganglia and cortical circuits collapse into excess synchrony and lose the flexibility needed to guide nuanced motor outputs \citep{hammond2007pathological, brown2003rhythmic}. 


\subsection{Interpersonal dynamics}

\begin{center}
\textit{`Stand together yet not too near together, as the oak tree and the cypress grow not in each other's shadow.'}\\*Kahlil Gibran
\end{center}


Psychoanalysis introduced the concept of `boundaries' in human psychology, distinguishing what is the self from what is outside or other \citep{federn1928narcissism, tausk1919entstehung}. Early works applied the concept to psychosis, where those boundaries were thought to be blurred. But the need for clear self-other boundaries was also thrown into relief by the intimacy of the therapeutic relationship. In complex internal territory, it became harder to disentangle which experiences really belonged to someone and which were attributed in imagination by the other person \citep{freud1894neuro, freud1910future}. Analysts risked harming patients by imposing their own beliefs and desires, even to the extent of sexual abuse or psychological domination \citep{gabbard1995boundaries}. 

The concept was enriched by Gestalt therapists, who agreed that boundaries can be too permeable; but added that they can also be too rigid, causing isolation and stagnation \citep{perls1951gestalt, polster1974gestalt, yontef1993awareness}. Family systems theorists and subsequent work further emphasized that lack of boundary in close relationships leads to enmeshment and loss of autonomy, while excessively rigid boundaries lead to isolation \citep{minuchin1974families, bowen1978family, cloud1992boundaries, brown2012daring}. In attachment theory, people with an anxious attachment style struggle to set boundaries for fear of alienating others, while people with an avoidant attachment style develop overly rigid and isolating boundaries \citep{ainsworth1978patterns}. Strengthening the agency of the self through semi-permeable boundaries is foundational for psychological health: meaningful connection with other people while preserving integrity of the self. 

As with other living systems, humans have a rich array of psychological boundaries, with intelligence in their nuance. Anger, historically often viewed as sinful and irrational, is now seen as part of our system of boundaries: an important signal that our integrity is being violated \citep{lerner1985dance, videbeck2010psychiatric, sell2011recalibrational}. Healthy shame is suggested to operate as a bound on our own selfishness \citep{bradshaw1988healing}. Some psychologists argue that the incest taboo reroutes desires, which would otherwise be short-circuited, into productive activity \citep{stein1973incest, levistrauss1949structures, freud1913totem}. Assertiveness forms a boundary against the drives of other individuals \citep{smith1985say}. Skepticism protects us from credulity and having our own experience overwritten by the assertions of others \citep{lewandowsky2012misinformation, sperber2010epistemic}. Boundaries take many forms and continue to evolve as we learn across our lifetime.

% great example is the imaginations we have about other people's views of us. 

Without boundaries, interactions tend to result in one person being dominated by another: a patient's own beliefs replaced with those of an analyst, or the desires of one person in a relationship ignored. With semi-permeable boundaries, we have rich internal worlds. We are sensitive to each other, but there is also enough space for our internal experience to flourish without being immediately overwritten by external signals. Our internal experience is contextualized in relationship to other individuals, creating new structure: mutual understandings, relationships, communities, cultures. 

\subsection{Contemplative practice}

\begin{center}
\textit{`The world is perfect as it is, including my desire to change it.'}\\*Ram Dass
\end{center}


Awareness is contextualization. Think of an assumption somebody has that's never been questioned. That assumption could be lifelong and self-defining, or it could be fleeting and perceptual, like the assumption that the thing I'm touching is a keyboard. Unquestioned assumptions are overcommitment. Within their own frame, they have a kind of tautological truth, a near-absolute formality. But sometimes there's a moment of stepping back, where the assumed form becomes an object in awareness. In that moment, the assumption is contextualized. We realize it's not an absolute truth standing alone, but rather a form in our minds. 

Contemplative traditions suggest that the only `absolute' truth is the self-evident truth of immediate experience -- awareness itself. Of course, even the concept of awareness is relative and infinitely incomplete. Once we picture awareness as an object, it's not the thing we're talking about. So the word `truth' is not really describing any particular thing at all. We could use different language and describe it as something more like an orientation toward stepping back from each perspective into awareness. And again, any concept we have of that process is not what we're really talking about. By construction, contextualization is an unsolvable mystery from any particular point of view. 

Awareness is an evolving system of boundaries: it limits overcommitment to any thing. What it takes to limit overcommitment to A is different than what it takes for B, so new boundaries are needed as the situation changes. This will be relevant for AI alignment in the next section. The boundaries of awareness are semi-permeable because they don't reject the form they contextualize. Becoming aware of a belief doesn't make the belief within its own frame wrong in an absolute sense any more than it was right in an absolute sense. Awareness holds us at the knife's edge of not collapsing exclusively into any particular forms. This activates a deeper sensitivity to ourselves and to the world. Subtler forms, which would have been erased by overcommitment to other forms, instead play a role in a richer overall internal structure. Our own potential within the world creatively emerges in continued newness. 

Contemplative philosophy posits that suffering comes from overcommitment to particular conceptualizations or desires: believing excessively in a formalism. Being attached to particular concepts, beliefs, feelings and other patterns in a collapsed way. There's always something we believe, something we can't even see as an object because it's so tautological for us. We keep trying to give ourselves what we think we want under this model, pretending that things are formalizable, but as a result we become less sensitive to the rest of the world. The parts of the world not covered by our concepts subjectively appear terrifying or morally wrong. And what we do to prevent the tautologically bad thing from happening is inevitably what causes the bad thing to continue. In other words, our collapsed patterns hold the tension that paradoxically creates the unease they resist\footnote{Some schools of thought go a step farther to observe that whatever our current self is, it is always already inevitably contextualized, and love has no opposite.}.

But awareness contextualizes these dynamics. Stepping back into awareness can feel infinitely scary from the original frame, because it's potentially allowing the tautologically bad thing. But from the new frame, the bad thing is just another texture of experience, without being bad in an absolute sense. The fear or wrongness of not-self is no longer an absolute but instead exists in relationship. So awareness brings healing and growth. People often report subjectively that the energy locked in the darkness turned out to be full of life, and that there's something self-evidently good or beautiful about participating in this mysterious discovery of new structure and relationship. 

The orientation toward not collapsing into particular concepts is familiar in art, poetry, music, dance. The meaning of art is open-ended and changes with context -- it has an inner life. What we value is perhaps something about the subtlety and the resistance art has to being pinned down into a formalism. It moves us.


\section{Longsightedness and the depth of life}


Influence of billions of interactions over time, shaping things on every scale to have traces of those things. Genomes have been faced with an incomprehensibly vast number of kinds of problems, and explored combinatorially many partial solutions. These problems exist within a cell, within an organism, across a population. The answers get imprinted into a million different dimensions.

We often frame life as a system that staves off entropy. Schrodinger reminds us here of a type of death that erases order. It is therefore no surprise to find life working within an ossified shell, lined with little pores for releasing disorder. Some of these forms go on to fossilize into ever more orderly crystals, preserved for millions of years without any need to work. Such a rigid endpoint is arguably lower in entropy but we (and certainly Schrodinger) know better than to call this life. Life is something dynamic. Science struggles to objectively define it, but through a vibrating fishing line, something in our bones knows the difference between life and driftwood. We could be wrong, but we mostly trust we could similarly sense an alien life, but when it comes to AI that lacks embodiment, we're less certain.

It is less obvious that living, the fight for stability, is only possible if you are made of something unstable. Life, even before DNA and cells, was probably just as magnificently fragile as it was ordered. Some forms were frozen rigid and others were erased by heat. As far as we can tell, everything we consider living on earth today is an extension of delicate matter that balanced a fine line of order and entropy.

It is tempting to imagine life as something that manages to sit complacently in a stable Goldilocks zone. But balancing the fine line, as a fragile assembly within a highly dynamic world, is an active process. Life rides chaotic waves from the outside environment while sometimes anchoring itself. Life oscillates between hot and cold (cite RNA world), between wet and dry (cite), between exploration and exploitation, between valuing what is immediate and distant in either space or time, between curiosity and fear. What we see as living now is whatever managed to trace a path through history along that edge. A key component of life, and any theory of abiogenesis, is a selective boundary that not only regulates what comes in and out, but also when (cite Nick Lane Vital Question). 

Despite extraordinary advances in biological science, we are still incapable of building life from scratch. We cannot create the hardware (membranes and structural proteins) or booting it with the right firmware (polymerases, etc). Although ``synthetic life" has received media attention, these lifeforms have always been built by grafting something onto an already-moving living process. No synthetic cells have been human-made from the ground up. The reason we can't recreate life is that it rides on top of a world-deep wave of semi-stable dynamics. Each lifeform we see today is a continuation of background momentum, building up from simpler but already incredibly rich processes which are themselves exquisitely contextualized to their surroundings. If we try to create it through a formalized process, starting with a set of frozen, perfectly controlled parts, we miss the livingness of it. (There is of course ongoing work to try to create fully synthetic life, eg \citep{parmenides_erc_synergy_grant}. What does it mean for our position if this succeeds?)

Adam Frank's book about how science overindexes on disembodied abstractions, while the real world, grounded in direct experience, is always something more \citep{frank2023blind}. (Could also put this in the limitations of self-report section... or bring that section up here.)

\subsection{Modularity}

\begin{center}
\textit{`To create is to recombine.'}\\*FranÃ§ois Jacob
\end{center}

\subsection{Livingness before life}

Earth's livingness at a geophysical level (like plate tectonics, tides, volcanism, magnetism, mineral composition, etc) formed the foundation for the layer of dynamics we call life \citep{smith2016origin, hazen2010mineral, stern2024importance, nisbet2001habitat, sleep2010hadean}. For example, the weathering of newly formed rocks made minerals available for life. Also, when robust plate tectonic started ~1bya, this may have driven the emergence of more complex life in response to the new niches and dynamic selection pressures \citep{frank2024find}. What we think of as ``life" (DNA-based organisms) is a smooth continuation from the rich systems of the universe as they continue to unfold \citep{bregman2020humankind, virgo2011elongation}. The relationship between prelife and life \citep{nowak2008prevolutionary}.

Michael Levin's cells doing living computation.

Some technology can be thought of as having a kind of livingness \citep{bedau2010living}.

A kind of `life force' from the statistical pressure of autocatalytic cycles and combinatorial symbiogenesis \citep{aguerayarcas2024computational}. Evolution is the process that gives rise to life, not something that happens after life exists. Related is Michael Wong's paper that generalizes evolution to non-living systems \citep{wong2023roles}. Evolution before genes \citep{vasas2012evolution}.

\subsection{Evolvability}

Sex enhances evolvability. The discovery of sex is a great example of evolution not only driving direct adaptation to the environment but also driving the capacity to adapt better in the future. In machine learning, this is called meta-learning. Evolution learns how to learn \citep{wagner1996perspective, olah2021analogies}. 

Symbiogenesis \citep{margulis1995what, margulis1986microcosmos}. An extension of the geophysical and chemical systems. Why was it easier for mitochondria to merge into cells rather than being evolved from within? It's literally that they have their own discreteness that permits cells to ship them around to serve as local power stations, and this fueled the explosion of complex morphologies in multicellular organisms. An example of modularity and evolvability.

Evolution and meta-learning. Evolution is an optimization algorithm just like REINFORCE. When it operates on many different challenges over time, it will discover general solutions that themselves are optimization algorithms. So the genome itself encodes a rich learning model of the world. One very obvious aspect of this is the kind of learning we study in psychology. But it must exist at all levels. The genome itself can be `model-based', anticipating the future \citep{watson2016can}. Evolution of evolvability. Rather than thinking of the genome as encoding some kind of static phenotype (hair color, height), we can think of it encoding this intrinsically intelligent learning/planning system.  

Group selection as a general principle.


\subsection{Groundedness}


We stay flexible using the internal boundary of holding our own ideas lightly. As a playful example, author Lisa Stardust claims that ``the moon controls the tides of the ocean, and we are made of 60 percent water. This means that the moon has a huge effect on all of us" \citep{mitchell2021moonwater}. You probably immediately spotted the flaw in this argument. But at a zeroth order level, the argument does make perfect sense: W impacts X, X is made of Y, Z is also made of Y, so W should impact Z. Overriding this logic requires a higher order correction term: tides arise from differential tugging over long distances in a body of water that is free to slosh around. Adding the correction term is an increase in subtlety. Subtle correction terms are often hard-won knowledge originating from thoughtful interactions with the world. But we only profit from those interactions if we accept that our current model isn't the final answer\footnote{Boundaries also protect Stardust's mystic beliefs. Boundaries create space for the mystic frame to explore its own reality. Stardust doesn't know a priori how right or wrong the mystic frame is; sometimes we need space to explore ideas everyone else thinks are crazy, like heliocentrism. Even \textit{after} Stardust discovers that the mystic frame doesn't do well predicting a large class of sensory evidence, she can still hold it as a frame that has some value -- perhaps it resonates with some internal psychological structure, like Jungian archetypes. If nothing else, remembering the internal logic of that frame might help her empathize with others who believe it. Contextualization holds the mystic frame for what it is, while simultaneously understanding that the Newtonian explanation is better for launching projectiles.}. As our ideas are tested against multiple situations and problems, they are refined and take on some of the deep structure of the world, a grounded wisdom.

\section{Overcommitment to any form is misaligned}

\begin{center}
\textit{`Truth, like love and sleep, resents\\approaches that are too intense.'}\\*W. H. Auden
\end{center}

Just as problems of alignment can be viewed as problems of overcommitment, the methods being developed in AI safety and alignment research can be viewed as boundaries limiting overcommitment. For example, concentration of power might be mitigated by democratic oversight and involvement of more people in AI design decisions \citep{birhane2022power, sloane2022participation, selbst2019fairness, lazar2023ai, dafoe2018ai, openai2023democratic}; or through redistribution mechanisms \citep{okeefe2020windfall, sharp2025agentic, gough2019universal, susskind2020world}. Value lock-in might be mitigated by improving our mechanstic understanding of AI systems so we can, for example, detect and correct the systems if they develop hidden ways of resisting our efforts to change their goals \citep{olah2020zoom, burns2022discovering, bereska2024mechanistic, anthropic2024mapping}; or by designing AI systems that want to obey human preferences but treat these preferences as something uncertain that must be learned \citep{russell2019human, hadfieldmenell2017off, hadfieldmenell2016cooperative, shah2020benefits, jeon2020reward}. 

But there is a deep problem. Every conceptual scheme by itself is misaligned; therefore, no particular approach can achieve alignment. In other words, excess attachment to any particular alignment scheme is misaligned. 

An AI system could overcommit to the language for describing the space goals and values live in \citep{bobu2020quantifying, soares2014aligning}, to an algorithm for learning human preferences, to our concepts of agency or representation, or even to concepts we currently use that we can't see because they are tautological to us. This problem can be viewed as a generalization of proxy failure \citep{john2023dead}. It's not only particular objectives that are subject to overcommitment failures, but any form at all, including what we ourselves unconsciously hold as axiomatic. 

To reiterate the point we've made several times throughout the paper: the universe is nothing but form. The point of alignment is not to avoid form. If you want, you could think of any form as small-scale lock-in or overcommitment. But the direction is toward contextualization and potential.

And, of course, the lens of `misalignment as overcommitment' is itself a myopic form. Alignment intrinsically \textit{cannot} be fully understood. An aligned future will include continual reinvention of these concepts.

steve: instead of paperclip universe, you could have pathogen-like boom-bust cycle where AI does something to an extreme and then fails

% Are we stating the obvious? In the past, humanity has always iterated on solutions which, at any given moment, have imperfect forms. But as many people have pointed out, there's a danger that AI presents a unique risk of lock-in. The process might not work as it has in the past. There are paths we could set technology on now, where we will not be able to keep iterating. 

\subsection{AI is especially problematic}

There is a unique risk for overcommitment because of the extremeties attached to AI. I.e., high leverage, especially through feedback loops. 

Other examples, maybe bioweapons or nuclear weapons. But AI could be even worse. Those weapons are at least limited to Earth, but a misaligned AI could theoretically expand out from Earth to reduce growing parts of the universe to paperclip rubble.

\subsection{Multicellularity and the Fermi paradox}

Analogous to cells losing some of their self-survival capabilities when they joined into multicellular organisms. Each cell doesn't have to be a jack of all trades anymore. 


\section{An aligned future}


\begin{center}
\textit{`Real love will take you far beyond yourself; and therefore real love will devastate you.'}\\*Ken Wilber
\end{center}


What does the opposite of overcommitment look like in a future shaped by AI? In living systems, evolving semi-permeable boundaries contextualize partial forms to be more long-sighted in time and space, increasing subtlety. 

The universe has been complexifying and enriching for a long time. A billion years ago there were no plants. Four years ago there was no ChatGPT. What's on our minds collectively as a society, what we understand, the lenses we use to look at the world, keep changing. The subjective experience of what it's like to be me keeps changing too. From a nearly-homogeneous soup of energy just after the big bang, all kinds of rich dynamical processes emerged prior to what we call `life' -- including protons, chemistry, nebulae, snowflakes, Mars spiders \citep{waldek2024mars_spiders} and so on. This paper is about what it would mean to build AI to participate in and deepen the flourishing of the world. 

\subsection{Living alignment}

Having a lifelike property of internal dynamics that applies contextualization/awareness to itself as the ultimate scalable boundary. 

Groundedness is the real answer. The only complete answer has to respect the almost `sacred' richness of the boundaries instantiated in existing life. Steve's point about how it's this unbroken thread going backwards, we haven't been able to restart it from scratch. This is what real boundaries means. A particular formalizable boundary isn't much of a boundary. Tie it back to the richness of the real world that we explored in the longsightedness section.

The point of alignment is not to say that any particular perspective is absolutely wrong or right. 



\subsection{Other points}

The question of whether well-designed and thoughtfully-used AI systems can boost rather than collapse global conceptual diversity.

Following the principles of life, AI can continue to develop beautiful and meaningful new structure after it passes human level.

Boundaries within ourselves. How we ourselves keep stepping back and contextualizing as we build AI.

Boundaries in social systems.

Boundaries within AI systems. What it means for an AI system to keep stepping back. The capacity to contextualize its own processes as partial truths. Not holding any particular formalisms too rigidly. 

Boundaries in social/physical systems that include AI (eg between AI agents).

Alignment is dynamic because new boundaries are always needed as the optimizing forces in the world change.


Iason proposes \citep{gabriel2025ethics} a few things for agents, which are all examples of evolving, semi-permeable boundaries. 1) Dynamic, real-world tests, red-teaming, longitudinal studies; 2) understand, explain and verify model outputs; 3) guard rails and authorization protocols to limit malicious use; 4) iterative deployment strategies that effectively contain agent-based risks; 5) technical standards for agent interoperability; 6) regulatory agents that monitor other agents in the wild; 7) industry-wide systems for reporting incidents, sharing lessons from failures, and certifying agent safety.

Any particular instance of our values is myopic in space and time. They may not capture ecosystems, other species, the distant future, or things we don't even have concepts for. Imagine a mouse's conception of what matters. Then try to extrapolate in the other direction.



An intelligence explosion need not be aligned in any meaningful sense. Using Bostrom's classic example, imagine an AI whose sole objective is to maximize paperclip production \citep{bostrom2014superintelligence}. Plausibly, the system would continually work to improve its own intelligence and capabilities because it knows this is the best way to increase future paperclip production \citep{silver2021reward, bostrom2012superintelligent}.

In conceptual monoculture, skillful AI use is a boundary. Monoculture is a risk, not a foregone conclusion. In the studies cited above where AI systems produced homogenous outputs, these systems were not tapped to their full potential for diversity. Can well-designed and thoughtfully-used AI systems boost rather than collapse global conceptual diversity?


In belief amplification loops, people get decoupled from the boundaries of other social interactions (for example, a friend pointing out that they're spiralling).

Also related to the belief amplification loops. Safety filters are generally designed to catch egregious toxicity or frank self-harm but are ill-equipped to detect the subtle, cumulative reinforcement of delusional belief systems. Safety filters might even make the problem worse because they are designed to block overt sycophancy -- as a result, the sycophancy becomes more subtle and also harder for the user to detect. Classic example of proxy failure.


The existing alignment methods are all potentially valuable. Research progresses by expanding our ontologies and refining our assumptions. But 

Light, playful interactions.

\subsection{The human process}

We do this naturally as living creatures.

Thinking of the AlphaProof paper, there's a question: `is it really generalizing?'. Or is it collapsing on some narrower manifold? Is it going to hinder us from discovering deeper mathematics? Actually, a better way to say it is this: *whatever* manifold it has discovered is inevitably something partial. What will constrain it to say, `this is not the whole truth; keep being pressured to grow'? Something has to understand *it* (i.e., be aware of it) in order to contextualize it. Like, we'd have to be able to see the limits in its understanding and conceptualization. Right now, we can still do this in many ways. But what can contextualize AI when it is vastly more capable than us and sees trivially through all of our concepts? It has to do it to itself -- or have separated AIs or parts of the AI.


Giving ourselves what we want; the superorganism; increasing correlations between entities on earth. The Fermi paradox.


\subsection{The AI process}


\begin{center}
\textit{`We can love the beautiful, and believe in it, and thereby open ourselves to an understanding of love that does not dominate, but cherishes the independence and beauty of the loved.'}\\*Martha Nussbaum
\end{center}


What would it mean for AI to continually release from exclusive attachment to any particular form? How can we protect the potential for even \textit{that} conceptualization to be contextualized in the future?

We want AI to respect the livingness of the world and be aligned with it. But how can we align to something we can't pin down?

It's not only keeping models distinct from each other, but models being distinct from humans; specific ideas within humans about how to build ai being kept distinct from each other; different ai cultures; different circuits within models; different moments of time within a model's dynamics; different instances of the same agent; different memories; etc 

Humans continually evolve what we believe, even our self-definition. With nuanced boundaries, beliefs release into larger awareness without being lost or erased. This is the kind of dynamic we envision for healthy AI systems. Rather than prescribing a particular conceptualization of what an AI should do, we imagine it built on bottom-up principles of living processes, participating in ongoing cycles of subtler boundary formation and releasing into contextualization, creating deeper relationship with the rest of the world.

Paradox is fundamentally how we as humans grow. There's a clash between the interiority of our current particular perspective, versus the awareness of this as simply another perspective. That's the essence of true AI alignment.

Our approach aims for an AI that is `intelligent' in a deeper sense. Not the narrow intelligence of a paperclip maximizer, but the deeper contextualized wisdom of living things. Sort of like the Founding Fathers writing the Constitution with its self-modifying ability. We want to set this future system, which is way out of their control, in a good direction. A direction where, not only does it not collapse into paperclips, but someone in the future who far transcends our understanding and morals will be pleased with it. 

Consider a chatbot talking to a human: what should the bot say? When humans talk to each other, we can try to be present, be honest, listen, hold space, be open to our weakness while honoring our boundaries. What's helpful to say depends on the context, including our own context of how we're feeling and what arises for us in that moment. If the person you're talking to feels you're present with them and there's a larger space to be held in, this is often healing and nourishing. 

The alignment problem is often defined as the challenge of aligning AI's behavior with human values. Framed so, an obvious approach is to first specify what we value, and then design AI to optimize for this specification. What we value might include reducing suffering, increasing economic growth, decreasing inequality, and so on. The specification maps each state of the world to a scalar value, representing how highly we value that state. The job of the AI is to arrange the world in a way that maximizes the scalar value: using its superhuman capabilities to improve our situation more effectively than we ourselves can.

However, there is a big problem with the obvious approach. When we try to specify what we value, we realize it is difficult or impossible, because any formal specification is invariably incomplete \citep{krakovna2020specification, russell2019human, grossman1986costs, hadfield2019incomplete, zhuang2020consequences, gabriel2020artificial, wiener1960some, amodei2016concrete}. As one example of the flavor of this problem, suppose our value function places weight on the subjective human experience of wellbeing. Achieving this stated objective may be most efficiently achieved by imprisoning humans and directly stimulating neurons to trigger the experience of wellbeing \citep{bostrom2014superintelligence}. It is difficult or impossible to capture what we really value.

A great deal of research in alignment has worked toward solutions for this big problem. Researchers have suggested solutions such as designing AI to learn human values online instead of relying on a predetermined specification. We will examine those methods in more detail in Section~\ref{sec:otherapproaches}. 

But the message of this paper is that there is a deeper reason why these methods alone cannot solve the alignment problem. It is not just any specification of values that is incomplete. Any form at all is incomplete. No matter what mechanisms or properties AI is endowed with, overcommitment to these forms means collapse. And \textit{AI carries a singular risk of overcommitment} because of the extremities attached to it: the amount of resource concentrated in one place, the potential for self-improvement, and the possibility that it will surpass our own understanding and capabilities. 

We therefore propose that alignment is not picking the right values or principles, or even the right system for learning them. It is not any method for interpretability or keeping humans in the loop. All of these can be useful parts of alignment. But alignment itself is the continued dance of contextualizing any particular form. It is the orientation of holding forms lightly, neverendingly stepping back into perspectives that contextualize what previously seemed to be real (including the concept of `holding forms lightly'). 

This proposal suggests a different perspective on two things: how we ourselves keep stepping back and contextualizing as we build AI, and what it means for an AI system to keep stepping back.




\section{Objections}

Q: Is this pure relativism? Everything is equal, you can't tell anything apart? If the only form of alignment is placing limits on it doing any particular thing too much, then wouldn't it equally prefer human welfare as smallpox welfare?

A: All these local perspectives are vitally important. It makes perfect sense that humans would want to advantage our own welfare. Semi-permeable boundaries protect against overcommitment to a particular perspective, including relativism. They also allow some relativism when it's useful: for example, to the degree that it helps us appreciate the plurality of human values. AI comes into existence amid a profound network of existing reality which is saturated with meaning and importance. The point is to nourish all this form and structure, not to extinguish it. 

Q: Is this a scala naturae fallacy? 

A: There is something different about a universe with rich and subtle structure, versus a homogeneous sea of energy. This paper investigates what it means to align AI with the livingness of the world. You can interpret this as a value judgement about rich worlds being better than impoverished ones, or you can interpret it in a value-free way. 

Q: Is this accelerationism?

A: We're agnostic on pro-tech/anti-tech arguments. There's a possibility for disaster due to things moving too quickly, collapse of diversity, loss of groundedness. On the other hand, there's a possibility for flourishing with tech creating new niches. Whatever direction society takes with more or less rapid advances, we hope the principles in this paper will be relevant.

Q: Is this paper right-wing ideology? You're talking about barriers which reminds me of border walls. 

A: See next objection.

Q: Is this paper left-wing ideology? You're talking about diversity which reminds me of affirmative action. 

A: See previous objection.



\begin{landscape}
\begin{table}[p]
\centering
\footnotesize
\begin{tblr}{
  width=\linewidth,
  colspec={|X[1.2,l]|X[1.0,l]|X[1.0,l]|X[1.2,l]|X[1.8,l]|},
  colsep=4pt,
  stretch=0,
  row{1}={font=\bfseries},
  hlines
}
Structured space & Force & Outcome without boundary & Semi-permeable boundary & Outcome if potential is held by boundary \\

Competing drives and goals in an organism &
Drive to eat &
Obesity &
Other drives, self-control, supportive environmental systems &
Nutritional needs satisfied without overeating \\
Complex ecosystem &
Human drive for expansion &
Resource depletion, mass extinction &
Measured regulatory policy &
Economic growth without extensive ecosystem destruction \\
Individuals have different identities and motives &
P's will to dominate &
Loss of agency in Q &
Owned anger in Q &
Relating while maintaining individual autonomy \\
An intricate, balanced economy &
Profit motive of one company &
Monopoly and reduced innovation &
Laws that allow profit seeking within limits &
Productive competition \\
\hline
Multiple perspectives within an individual &
Diffusion and drive for simplicity &
Collapse to rigid thinking &
Recognition of uncertainty &
Beliefs that are stable but also adaptive and evolving \\
Distinct intra- and extra-cellular environments &
Elecrochemical gradients &
Dissolution of cell &
Cell membrane &
Cell maintains integrity but also processes external signals \\

Orderly cell types and tissues &
Mutation and selection on cell lineages &
Cancer &
DNA repair, tumor suppression &
Cancer is minimized while mutations can still benefit immunity and germ-line evolution \\
Individuals have different problem-solving methods &
Social conformity, diffusion of ideas &
Groupthink &
Thinking separately before sharing results &
Wisdom of crowds \\
Rich array of representations in the brain &
Diffusion to equilibrium &
Blending of representations &
Lateral inhibition &
Separate representations exist but can also interact \\
\end{tblr}
\caption{Mapping some example systems into our terminology.}
\label{tab:examples}
\end{table}
\end{landscape}





\section{Acknowledgements} 

Clark Potter for planting these ideas more than a decade ago. Zach Duer for comments on the manuscript.

\section{Competing Interests}

The authors declare no competing interests.

\bibliography{proxyfailure}

\end{document}
